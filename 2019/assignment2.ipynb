{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TueSNLP 2019 - Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying languages\n",
    "\n",
    "The assignment and data are available at https://snlp2019.github.io/a2/\n",
    "\n",
    "The data is a subset of WALS (https://wals.info/) and it consists of a tab-separated file with a `family` column containing the family of a language and other columns containing features for the language (notice: one language per row, first row is header and first two columns not essential)\n",
    "\n",
    "The goal of the assignment is to learn language classification based on typological features.\n",
    "\n",
    "(as per assignment instructions, our code roughly follows the template(s) provided)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1. Encoding the data\n",
    "\n",
    "We write a function `encode()` which reads the data file and returns two numpy arrays, `labels` and `feature`; the former is the list of language families, the latter is a 2d array with as many rows as there are labels, where each row is the concatenation of the one-hot encodings of the features corresponding to the label. `NA`s and unknown values will be mapped to vectors of `0`s.\n",
    "\n",
    "We do this step-by-step, then we put everything together in a function definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data and take a look\n",
    "df = pd.read_csv(\"data/wals-train.tsv\", sep = \"\\t\").fillna(\"NA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "  lcode              lname        family  \\\n0   amh            Amharic  Afro-Asiatic   \n1   arz  Arabic (Egyptian)  Afro-Asiatic   \n2   bej               Beja  Afro-Asiatic   \n3   heb    Hebrew (Modern)  Afro-Asiatic   \n4   irk              Iraqw  Afro-Asiatic   \n\n  143A Order of Negative Morpheme and Verb 143F Postverbal Negative Morphemes  \\\n0                        14 ObligDoubleNeg                          2 [V-Neg]   \n1                          15 OptDoubleNeg                          2 [V-Neg]   \n2                                3 [Neg-V]                             4 None   \n3                                   1 NegV                             4 None   \n4                                4 [V-Neg]                          2 [V-Neg]   \n\n  143G Minor morphological means of signaling negation  \\\n0                                             4 None     \n1                                             4 None     \n2                                             4 None     \n3                                             4 None     \n4                                             4 None     \n\n  82A Order of Subject and Verb 143E Preverbal Negative Morphemes  \\\n0                          1 SV                         2 [Neg-V]   \n1                          1 SV                    3 NegV&amp;[Neg-V]   \n2                          1 SV                         2 [Neg-V]   \n3                          1 SV                            1 NegV   \n4                          1 SV                            4 None   \n\n  83A Order of Object and Verb 85A Order of Adposition and Noun Phrase  ...  \\\n0                         1 OV                     4 No dominant order  ...   \n1                         2 VO                          2 Prepositions  ...   \n2                         1 OV                         1 Postpositions  ...   \n3                         2 VO                          2 Prepositions  ...   \n4                         1 OV                          2 Prepositions  ...   \n\n  19A Presence of Uncommon Consonants 3A Consonant-Vowel Ratio  \\\n0                              1 None        4 Moderately high   \n1                       4 Pharyngeals        4 Moderately high   \n2                              1 None                3 Average   \n3                                  NA                       NA   \n4                       4 Pharyngeals                   5 High   \n\n          8A Lateral Consonants            6A Uvular Consonants  \\\n0  2 /l/, no obstruent laterals                          1 None   \n1  2 /l/, no obstruent laterals  4 Uvular stops and continuants   \n2  2 /l/, no obstruent laterals                          1 None   \n3                            NA                              NA   \n4   4 /l/ and lateral obstruent             2 Uvular stops only   \n\n  18A Absence of Common Consonants 69A Position of Tense-Aspect Affixes  \\\n0                    1 All present                         4 Mixed type   \n1                    1 All present                         4 Mixed type   \n2                    1 All present              2 Tense-aspect suffixes   \n3                               NA                         4 Mixed type   \n4                    1 All present         5 No tense-aspect inflection   \n\n  112A Negative Morphemes 107A Passive Constructions  \\\n0       6 Double negation                  1 Present   \n1     2 Negative particle                  1 Present   \n2        1 Negative affix                  1 Present   \n3     2 Negative particle                  1 Present   \n4        1 Negative affix                  1 Present   \n\n  48A Person Marking on Adpositions 1A Consonant Inventories  \n0                   3 Pronouns only       4 Moderately large  \n1                   3 Pronouns only       4 Moderately large  \n2               2 No person marking                3 Average  \n3                   3 Pronouns only                       NA  \n4               2 No person marking       4 Moderately large  \n\n[5 rows x 33 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>lcode</th>\n      <th>lname</th>\n      <th>family</th>\n      <th>143A Order of Negative Morpheme and Verb</th>\n      <th>143F Postverbal Negative Morphemes</th>\n      <th>143G Minor morphological means of signaling negation</th>\n      <th>82A Order of Subject and Verb</th>\n      <th>143E Preverbal Negative Morphemes</th>\n      <th>83A Order of Object and Verb</th>\n      <th>85A Order of Adposition and Noun Phrase</th>\n      <th>...</th>\n      <th>19A Presence of Uncommon Consonants</th>\n      <th>3A Consonant-Vowel Ratio</th>\n      <th>8A Lateral Consonants</th>\n      <th>6A Uvular Consonants</th>\n      <th>18A Absence of Common Consonants</th>\n      <th>69A Position of Tense-Aspect Affixes</th>\n      <th>112A Negative Morphemes</th>\n      <th>107A Passive Constructions</th>\n      <th>48A Person Marking on Adpositions</th>\n      <th>1A Consonant Inventories</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>amh</td>\n      <td>Amharic</td>\n      <td>Afro-Asiatic</td>\n      <td>14 ObligDoubleNeg</td>\n      <td>2 [V-Neg]</td>\n      <td>4 None</td>\n      <td>1 SV</td>\n      <td>2 [Neg-V]</td>\n      <td>1 OV</td>\n      <td>4 No dominant order</td>\n      <td>...</td>\n      <td>1 None</td>\n      <td>4 Moderately high</td>\n      <td>2 /l/, no obstruent laterals</td>\n      <td>1 None</td>\n      <td>1 All present</td>\n      <td>4 Mixed type</td>\n      <td>6 Double negation</td>\n      <td>1 Present</td>\n      <td>3 Pronouns only</td>\n      <td>4 Moderately large</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>arz</td>\n      <td>Arabic (Egyptian)</td>\n      <td>Afro-Asiatic</td>\n      <td>15 OptDoubleNeg</td>\n      <td>2 [V-Neg]</td>\n      <td>4 None</td>\n      <td>1 SV</td>\n      <td>3 NegV&amp;[Neg-V]</td>\n      <td>2 VO</td>\n      <td>2 Prepositions</td>\n      <td>...</td>\n      <td>4 Pharyngeals</td>\n      <td>4 Moderately high</td>\n      <td>2 /l/, no obstruent laterals</td>\n      <td>4 Uvular stops and continuants</td>\n      <td>1 All present</td>\n      <td>4 Mixed type</td>\n      <td>2 Negative particle</td>\n      <td>1 Present</td>\n      <td>3 Pronouns only</td>\n      <td>4 Moderately large</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>bej</td>\n      <td>Beja</td>\n      <td>Afro-Asiatic</td>\n      <td>3 [Neg-V]</td>\n      <td>4 None</td>\n      <td>4 None</td>\n      <td>1 SV</td>\n      <td>2 [Neg-V]</td>\n      <td>1 OV</td>\n      <td>1 Postpositions</td>\n      <td>...</td>\n      <td>1 None</td>\n      <td>3 Average</td>\n      <td>2 /l/, no obstruent laterals</td>\n      <td>1 None</td>\n      <td>1 All present</td>\n      <td>2 Tense-aspect suffixes</td>\n      <td>1 Negative affix</td>\n      <td>1 Present</td>\n      <td>2 No person marking</td>\n      <td>3 Average</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>heb</td>\n      <td>Hebrew (Modern)</td>\n      <td>Afro-Asiatic</td>\n      <td>1 NegV</td>\n      <td>4 None</td>\n      <td>4 None</td>\n      <td>1 SV</td>\n      <td>1 NegV</td>\n      <td>2 VO</td>\n      <td>2 Prepositions</td>\n      <td>...</td>\n      <td>NA</td>\n      <td>NA</td>\n      <td>NA</td>\n      <td>NA</td>\n      <td>NA</td>\n      <td>4 Mixed type</td>\n      <td>2 Negative particle</td>\n      <td>1 Present</td>\n      <td>3 Pronouns only</td>\n      <td>NA</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>irk</td>\n      <td>Iraqw</td>\n      <td>Afro-Asiatic</td>\n      <td>4 [V-Neg]</td>\n      <td>2 [V-Neg]</td>\n      <td>4 None</td>\n      <td>1 SV</td>\n      <td>4 None</td>\n      <td>1 OV</td>\n      <td>2 Prepositions</td>\n      <td>...</td>\n      <td>4 Pharyngeals</td>\n      <td>5 High</td>\n      <td>4 /l/ and lateral obstruent</td>\n      <td>2 Uvular stops only</td>\n      <td>1 All present</td>\n      <td>5 No tense-aspect inflection</td>\n      <td>1 Negative affix</td>\n      <td>1 Present</td>\n      <td>2 No person marking</td>\n      <td>4 Moderately large</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 33 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 142
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The easiest part is to extract the labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df[\"family\"].to_numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "&lt;bound method IndexOpsMixin.to_numpy of 0         Afro-Asiatic\n1         Afro-Asiatic\n2         Afro-Asiatic\n3         Afro-Asiatic\n4         Afro-Asiatic\n            ...       \n84    Trans-New Guinea\n85         Uto-Aztecan\n86         Uto-Aztecan\n87         Uto-Aztecan\n88         Uto-Aztecan\nName: family, Length: 89, dtype: object&gt;\n"
    }
   ],
   "source": [
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 89 families in the labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to one-hot encode the 30 columns of features. We need a procedure to encode an array of values, which also saves the coding dictionary for that array, so, for example, we can encode the training data first, and then use the same coding scheme to encode the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a one-hot encoding function, it also returns the used coding dictionary\n",
    "def onehot_encoder(input_array):\n",
    "\tencoding_dict = {}\n",
    "\tunique_elements = list(set(input_array)) # the \"vocabulary\" of the array\n",
    "\n",
    "\t# first, put together the encoding dictionary:\n",
    "\tfor i in range(0, len(unique_elements)): # for each element of the input array:\n",
    "\t\tencoded_element = [0]*len(unique_elements) # initialize an array of 0s\n",
    "\t\tcurrent_element = unique_elements[i]\n",
    "\t\tif not current_element in [\"NA\"]: # NAs and similar should be vectors of 0s\n",
    "\t\t\tencoded_element[i] = 1 # the position corresponding to the current index should be set to 1\n",
    "\t\tencoding_dict[current_element] = encoded_element # associate each element with its coding, into the dictionary\n",
    "\t\t\t\n",
    "\t# next use the dictionary to encode the input array\n",
    "\toutput_array = [encoding_dict[element] for element in input_array]\n",
    "\n",
    "\treturn(encoding_dict, output_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_a = [\"a\", \"b\", \"NA\", \"c\", \"NA\", \"d\", \"a\", \"b\", \"f\", \"NA\", \"c\", \"f\", \"a\", \"z\", \"z\", \"z\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "enc_dict, out_arr = onehot_encoder(input_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{&#39;b&#39;: [1, 0, 0, 0, 0, 0, 0],\n &#39;c&#39;: [0, 1, 0, 0, 0, 0, 0],\n &#39;d&#39;: [0, 0, 1, 0, 0, 0, 0],\n &#39;z&#39;: [0, 0, 0, 1, 0, 0, 0],\n &#39;a&#39;: [0, 0, 0, 0, 1, 0, 0],\n &#39;f&#39;: [0, 0, 0, 0, 0, 1, 0],\n &#39;NA&#39;: [0, 0, 0, 0, 0, 0, 0]}"
     },
     "metadata": {},
     "execution_count": 148
    }
   ],
   "source": [
    "enc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[[0, 0, 0, 0, 1, 0, 0],\n [1, 0, 0, 0, 0, 0, 0],\n [0, 0, 0, 0, 0, 0, 0],\n [0, 1, 0, 0, 0, 0, 0],\n [0, 0, 0, 0, 0, 0, 0],\n [0, 0, 1, 0, 0, 0, 0],\n [0, 0, 0, 0, 1, 0, 0],\n [1, 0, 0, 0, 0, 0, 0],\n [0, 0, 0, 0, 0, 1, 0],\n [0, 0, 0, 0, 0, 0, 0],\n [0, 1, 0, 0, 0, 0, 0],\n [0, 0, 0, 0, 0, 1, 0],\n [0, 0, 0, 0, 1, 0, 0],\n [0, 0, 0, 1, 0, 0, 0],\n [0, 0, 0, 1, 0, 0, 0],\n [0, 0, 0, 1, 0, 0, 0]]"
     },
     "metadata": {},
     "execution_count": 149
    }
   ],
   "source": [
    "out_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it on a column of the df:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_dict, enc_col = onehot_encoder(df.iloc[:, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0    14 ObligDoubleNeg\n1      15 OptDoubleNeg\n2            3 [Neg-V]\n3               1 NegV\n4            4 [V-Neg]\n5               2 VNeg\n6               1 NegV\n7               1 NegV\n8    14 ObligDoubleNeg\n9            4 [V-Neg]\nName: 143A Order of Negative Morpheme and Verb, dtype: object\n"
    }
   ],
   "source": [
    "print(df.iloc[0:10, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{&#39;6 Type 1 / Type 2&#39;: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n &#39;11 Type 3 / Type 4&#39;: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n &#39;1 NegV&#39;: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n &#39;2 VNeg&#39;: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n &#39;10 Type 2 / Type 4&#39;: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n &#39;4 [V-Neg]&#39;: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n &#39;3 [Neg-V]&#39;: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n &#39;14 ObligDoubleNeg&#39;: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n &#39;15 OptDoubleNeg&#39;: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n &#39;NA&#39;: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n &#39;8 Type 1 / Type 4&#39;: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]}"
     },
     "metadata": {},
     "execution_count": 152
    }
   ],
   "source": [
    "enc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]]"
     },
     "metadata": {},
     "execution_count": 153
    }
   ],
   "source": [
    "enc_col[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears to be working; let's apply it to every column (except from first three):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_features = df.iloc[:, 3:].apply(onehot_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "143A Order of Negative Morpheme and Verb                ({&#39;6 Type 1 / Type 2&#39;: [1, 0, 0, 0, 0, 0, 0, 0...\n143F Postverbal Negative Morphemes                      ({&#39;2 [V-Neg]&#39;: [1, 0, 0, 0, 0], &#39;1 VNeg&#39;: [0, ...\n143G Minor morphological means of signaling negation    ({&#39;1 NegTone&#39;: [1, 0, 0], &#39;NA&#39;: [0, 0, 0], &#39;4 ...\n82A Order of Subject and Verb                           ({&#39;2 VS&#39;: [1, 0, 0], &#39;1 SV&#39;: [0, 1, 0], &#39;3 No ...\n143E Preverbal Negative Morphemes                       ({&#39;2 [Neg-V]&#39;: [1, 0, 0, 0, 0], &#39;1 NegV&#39;: [0, ...\ndtype: object"
     },
     "metadata": {},
     "execution_count": 155
    }
   ],
   "source": [
    "encoded_features.head()"
   ]
  },
  {
   "source": [
    "The object `encoded_features` is a list of pairs `(encoding_dictionary, encoded_feature)`, one for each feauture, i.e. the first element of each pair is the dictionary used to encode the corresponding feature, and the second element is the resulting, encoded, feature. So for example:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "({&#39;6 Type 1 / Type 2&#39;: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n  &#39;11 Type 3 / Type 4&#39;: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n  &#39;1 NegV&#39;: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n  &#39;2 VNeg&#39;: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n  &#39;10 Type 2 / Type 4&#39;: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n  &#39;4 [V-Neg]&#39;: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n  &#39;3 [Neg-V]&#39;: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n  &#39;14 ObligDoubleNeg&#39;: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n  &#39;15 OptDoubleNeg&#39;: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n  &#39;NA&#39;: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n  &#39;8 Type 1 / Type 4&#39;: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]},\n [[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n  [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n  [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n  [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n  [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n  [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n  [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n  [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n  [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n  [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n  [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n  [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n  [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n  [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n  [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]])"
     },
     "metadata": {},
     "execution_count": 156
    }
   ],
   "source": [
    "encoded_features[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, for each row `i` of the original dataset, we take the `i`-th element of the second element of each pair in `encoded_features`, obtaining a vector of (encoded) features for the row, and then we flatten it into a vector of 0s and 1s. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "ith_row = [feature[1][i] for feature in encoded_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n [1, 0, 0, 0, 0],\n [0, 0, 1],\n [0, 1, 0],\n [1, 0, 0, 0, 0],\n [0, 0, 1, 0],\n [0, 1, 0, 0, 0],\n [0, 1, 0, 0],\n [0, 1, 0, 0, 0, 0],\n [0, 0, 1, 0],\n [0, 0, 0, 0, 0, 1],\n [1, 0, 0, 0, 0, 0, 0],\n [0, 0, 0, 1, 0, 0],\n [1, 0, 0, 0, 0],\n [0, 0, 1, 0, 0, 0],\n [1, 0, 0, 0],\n [1, 0, 0, 0],\n [1, 0, 0, 0, 0, 0],\n [1, 0, 0, 0, 0, 0, 0],\n [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n [1, 0, 0, 0, 0, 0],\n [0, 0, 0, 0, 0, 1],\n [0, 0, 0, 1, 0, 0],\n [1, 0, 0, 0, 0],\n [0, 0, 1, 0],\n [0, 0, 1, 0, 0, 0],\n [0, 0, 1, 0, 0, 0],\n [0, 0, 1],\n [0, 0, 0, 0, 1],\n [0, 1, 0, 0, 0, 0]]"
     },
     "metadata": {},
     "execution_count": 158
    }
   ],
   "source": [
    "ith_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "ith_row_flat = np.array([item for element in ith_row for item in element])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,\n       1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n       1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,\n       0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n       0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0])"
     },
     "metadata": {},
     "execution_count": 160
    }
   ],
   "source": [
    "ith_row_flat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we do this for every row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.empty([89, 168]) # initialize empty matrix with appropriate dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in range(0, features.shape[0]):\n",
    "    ith_row = [feature[1][row] for feature in encoded_features]\n",
    "    features[row] = np.array([item for element in ith_row for item in element])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 1., ..., 0., 0., 1.],\n       [0., 0., 1., ..., 0., 0., 0.],\n       [0., 0., 1., ..., 1., 0., 0.]])"
     },
     "metadata": {},
     "execution_count": 163
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can put everything together in a function called `encode()` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(path_to_file, return_dictionaries = False): # added an option to return the set of encoding dictionaries as well\n",
    "    # read the data\n",
    "    df = pd.read_csv(path_to_file, sep = \"\\t\").fillna(\"NA\")\n",
    "\n",
    "    # labels\n",
    "    labels = df[\"family\"].to_numpy\n",
    "\n",
    "    # features\n",
    "    encoded_features = df.iloc[:, 3:].apply(onehot_encoder)\n",
    "    features = np.empty([89, 168])\n",
    "    for row in range(0, features.shape[0]):\n",
    "        ith_row = [feature[1][row] for feature in encoded_features]\n",
    "        features[row] = np.array([item for element in ith_row for item in element])\n",
    "\n",
    "    if return_dictionaries:\n",
    "        dictionaries = [encoded_features[i][0] for i in range(0, len(encoded_features))]    \n",
    "        return(features, labels, dictionaries)    \n",
    "    else:\n",
    "        return(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, labels, dictionaries = encode(\"data/wals-train.tsv\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,\n        0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n        1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n        1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n        1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n        0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n        1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n        0., 1., 0., 1., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0.,\n        0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,\n        0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1.,\n        0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,\n        0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n        0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n        0., 1., 0., 1., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n        0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n        0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n        0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0.,\n        0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n        0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0.,\n        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1.,\n        0., 0., 1., 0., 0., 0., 0., 0.]])"
     },
     "metadata": {},
     "execution_count": 174
    }
   ],
   "source": [
    "features[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "&lt;bound method IndexOpsMixin.to_numpy of 0         Afro-Asiatic\n1         Afro-Asiatic\n2         Afro-Asiatic\n3         Afro-Asiatic\n4         Afro-Asiatic\n            ...       \n84    Trans-New Guinea\n85         Uto-Aztecan\n86         Uto-Aztecan\n87         Uto-Aztecan\n88         Uto-Aztecan\nName: family, Length: 89, dtype: object&gt;"
     },
     "metadata": {},
     "execution_count": 176
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[{&#39;6 Type 1 / Type 2&#39;: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n  &#39;11 Type 3 / Type 4&#39;: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n  &#39;1 NegV&#39;: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n  &#39;2 VNeg&#39;: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n  &#39;10 Type 2 / Type 4&#39;: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n  &#39;4 [V-Neg]&#39;: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n  &#39;3 [Neg-V]&#39;: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n  &#39;14 ObligDoubleNeg&#39;: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n  &#39;15 OptDoubleNeg&#39;: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n  &#39;NA&#39;: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n  &#39;8 Type 1 / Type 4&#39;: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]},\n {&#39;2 [V-Neg]&#39;: [1, 0, 0, 0, 0],\n  &#39;1 VNeg&#39;: [0, 1, 0, 0, 0],\n  &#39;3 VNeg&amp;[V-Neg]&#39;: [0, 0, 1, 0, 0],\n  &#39;NA&#39;: [0, 0, 0, 0, 0],\n  &#39;4 None&#39;: [0, 0, 0, 0, 1]},\n {&#39;1 NegTone&#39;: [1, 0, 0], &#39;NA&#39;: [0, 0, 0], &#39;4 None&#39;: [0, 0, 1]}]"
     },
     "metadata": {},
     "execution_count": 175
    }
   ],
   "source": [
    "dictionaries[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works!"
   ]
  },
  {
   "source": [
    "### Exercise 2. Training a simple classifier"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38264bitcf06a54604fa462da990d50073f42498",
   "display_name": "Python 3.8.2 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}