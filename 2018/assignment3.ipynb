{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TueSNLP - Assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language identification\n",
    "The assignment is available here: https://snlp2018.github.io/assignments.html."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "This exercise is about creating a dataset of sentences in different languages starting from ids of tweets collected during the class. We don't have access to the private repo where the tweets had been saved, so we try to keep the spirit of the exercise using data from http://tatoeba.org a crowd-sourced collection of sentences and translations.\n",
    "\n",
    "In particular, http://downloads.tatoeba.org/exports/sentences.tar.bz2 contains ~8 milion sentences each with corresponding language code. In order to mimic the dataset originally provided for the assignment, we select 30 languages at random and pick a certain number of random sentences for each language. We use the majority of sentences to build the development set and the remainder to build an evaluation set which will be used to evaluate our model(s). Precise quantities and ratio will be determined after we explore the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>lang</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>cmn</td>\n",
       "      <td>我們試試看！</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>cmn</td>\n",
       "      <td>我该去睡觉了。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>cmn</td>\n",
       "      <td>你在干什麼啊？</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>cmn</td>\n",
       "      <td>這是什麼啊？</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>cmn</td>\n",
       "      <td>今天是６月１８号，也是Muiriel的生日！</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id lang                sentence\n",
       "0   1  cmn                  我們試試看！\n",
       "1   2  cmn                 我该去睡觉了。\n",
       "2   3  cmn                 你在干什麼啊？\n",
       "3   4  cmn                  這是什麼啊？\n",
       "4   5  cmn  今天是６月１８号，也是Muiriel的生日！"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read data\n",
    "full_data_raw = pd.read_csv(\"data/sentences.csv\", sep = \"\\t\", names = [\"id\", \"lang\", \"sentence\"])\n",
    "full_data_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't need sentence id, we can drop the column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data_raw = full_data_raw.drop(\"id\", axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lang            349\n",
       "sentence    8112291\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many unique languages?\n",
    "full_data_raw.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pick 30 languages at random, then filter the df:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sna', 'pap', 'bzt', 'ngt', 'ksh', 'lou', 'rus', 'hun', 'dws', 'oss', 'nch', 'que', 'ara', 'bel', 'ldn', 'fuv', 'otk', 'ile', 'jav', 'mah', 'ssw', 'tgk', 'cho', 'grc', 'gbm', 'sun', 'mfe', 'mar', 'nau', 'avk']\n"
     ]
    }
   ],
   "source": [
    "random.seed(2)\n",
    "languages = random.sample(set(full_data_raw[\"lang\"]), 30)\n",
    "print(languages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `\\\\N` is suspicious, let's see what it corresponds to in the df:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lang</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6095954</th>\n",
       "      <td>\\N</td>\n",
       "      <td>Sābuku mamayamin, niyaꞋ takiteku manaꞋul magla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6104067</th>\n",
       "      <td>\\N</td>\n",
       "      <td>Kataau kano koson i Ama' min pana mataau!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6310672</th>\n",
       "      <td>\\N</td>\n",
       "      <td>Нуӈан дэмэрипчут дылви амаскиви донӈорочон.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6310678</th>\n",
       "      <td>\\N</td>\n",
       "      <td>Чикчакун дочадяран, мудана ачинди иргикэндиви ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6310683</th>\n",
       "      <td>\\N</td>\n",
       "      <td>Том сома дэмэр куӈакан бичэн.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        lang                                           sentence\n",
       "6095954   \\N  Sābuku mamayamin, niyaꞋ takiteku manaꞋul magla...\n",
       "6104067   \\N          Kataau kano koson i Ama' min pana mataau!\n",
       "6310672   \\N        Нуӈан дэмэрипчут дылви амаскиви донӈорочон.\n",
       "6310678   \\N  Чикчакун дочадяран, мудана ачинди иргикэндиви ...\n",
       "6310683   \\N                      Том сома дэмэр куӈакан бичэн."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter df\n",
    "full_data_raw[full_data_raw[\"lang\"] == \"\\\\N\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears to be a placeholder for missing language codes. Let's filter out the corresponding rows:\n",
    "\n",
    "(eventually, if our classifier work well enough, we will be able to use it to infer the missing language codes...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lang            348\n",
       "sentence    8112201\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_data = full_data_raw[full_data_raw[\"lang\"] != \"\\\\N\"]\n",
    "filtered_data.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't know how many sentences there are for each language. Before sampling 30 languages, let's remove those with less than 100 sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lang</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lang</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>abk</th>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>acm</th>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ady</th>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>afb</th>\n",
       "      <td>1</td>\n",
       "      <td>133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>afh</th>\n",
       "      <td>1</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      lang  sentence\n",
       "lang                \n",
       "abk      1        26\n",
       "acm      1        49\n",
       "ady      1        31\n",
       "afb      1       133\n",
       "afh      1        79"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_count = filtered_data.groupby(\"lang\").nunique() # group by language and count sentences per group\n",
    "sentence_count.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "178"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract only languages with >99 sentences\n",
    "languages = sentence_count[sentence_count[\"sentence\"] > 99].index.values\n",
    "len(languages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Among these, we sample 30:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bar', 'cbk', 'bul', 'ldn', 'eus', 'wuu', 'kab', 'hrx', 'tha', 'got', 'vol', 'ast', 'swe', 'eng', 'nds', 'mar', 'prg', 'lit', 'sah', 'nob', 'pol', 'ido', 'urd', 'arz', 'lfn', 'ori', 'kaz', 'lvs', 'mya', 'rom']\n"
     ]
    }
   ],
   "source": [
    "random.seed(2)\n",
    "languages = random.sample(list(languages), 30)\n",
    "print(languages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's pick 100 sentences at random for each language:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lang</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>arz</td>\n",
       "      <td>كل الناس بتحب المكان ده.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>arz</td>\n",
       "      <td>كله محصل بعضه.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>arz</td>\n",
       "      <td>كارلوس طلع الجبل.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>arz</td>\n",
       "      <td>مفيش أي مشاكل.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>arz</td>\n",
       "      <td>معدش بيشوف.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  lang                  sentence\n",
       "0  arz  كل الناس بتحب المكان ده.\n",
       "1  arz            كله محصل بعضه.\n",
       "2  arz         كارلوس طلع الجبل.\n",
       "3  arz            مفيش أي مشاكل.\n",
       "4  arz               معدش بيشوف."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_data = filtered_data[filtered_data[\"lang\"].isin(languages)] # filter based on list of sampled languages\n",
    "# we apply a sampling function groupwise\n",
    "sampled_data = sampled_data.groupby(\"lang\").apply(lambda x : x.sample(100, random_state = 2)).reset_index(drop=True)\n",
    "sampled_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's shuffle this and remove the bottom 10% of rows, which we'll save as evaluation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lang</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ldn</td>\n",
       "      <td>Meháya rul woho wa.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eng</td>\n",
       "      <td>I want to borrow some money.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bar</td>\n",
       "      <td>Ohne Schmäh?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>vol</td>\n",
       "      <td>Fat Tomasa äbinom sanel.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ldn</td>\n",
       "      <td>Bíid thad dóyom le leyóoth wa, Thom.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  lang                              sentence\n",
       "0  ldn                   Meháya rul woho wa.\n",
       "1  eng          I want to borrow some money.\n",
       "2  bar                          Ohne Schmäh?\n",
       "3  vol              Fat Tomasa äbinom sanel.\n",
       "4  ldn  Bíid thad dóyom le leyóoth wa, Thom."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_data = sampled_data.sample(n = sampled_data.shape[0], random_state = 2).reset_index(drop=True)\n",
    "sampled_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_df = sampled_data.head(int(sampled_data.shape[0]*9/10))\n",
    "eval_df = sampled_data.tail(int(sampled_data.shape[0]*1/10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write both to disk for later use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_df.to_csv(\"data/assignment3-dev.tsv\", sep = \"\\t\", header = False, index = False)\n",
    "eval_df.to_csv(\"data/assignment3-eval.tsv\", sep = \"\\t\", header = False, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This exercise is about feature extraction. Each sentence is tokenized at the level of character bigrams. Then each sentence is represented as an array of counts of bigrams, for each bigram in the dataset (so a very sparse array, the vast majority of entries will be 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to extract character-level bigrams\n",
    "def sentence_tokenizer(in_string): # takes a string as input\n",
    "    tknzd_string = [] # initialize empty output\n",
    "    for i in range(0, len(in_string) - 1):\n",
    "        tknzd_string.append(in_string[i] + in_string[i+1]) # each bigram is a character followed by the next one\n",
    "    tknzd_string.insert(0, \"<BOS>\" + tknzd_string[0][0]) # first bigram is always <BOS>+first character\n",
    "    tknzd_string.append(tknzd_string[-1][-1] + \"<EOS>\") # last bigram is always last character+<EOS>\n",
    "    return(tknzd_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<BOS>t', 'ti', 'i ', ' c', 'ch', 'he', 'e ', ' t', 'ti', 'i ', ' t', 'ta', 'ac', 'ch', 'hi', 'i ', ' i', 'i ', ' t', 'ta', 'ac', 'c ', ' t', 'ta', 'ac', 'ca', 'am', 'm ', ' i', 'i ', ' t', 'ta', 'ac', 'c ', ' a', 'a ', ' m', 'mi', 'i<EOS>']\n"
     ]
    }
   ],
   "source": [
    "in_string = \"ti che ti tachi i tac tacam i tac a mi\"\n",
    "out_string = sentence_tokenizer(in_string)\n",
    "print(out_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to count bigrams in a sentence\n",
    "def bigram_counter(in_string):\n",
    "    tknzd_string = sentence_tokenizer(in_string)\n",
    "    bigrams, counts = np.unique(tknzd_string, return_counts=True)\n",
    "    return(bigrams, counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([' a', ' c', ' i', ' m', ' t', '<BOS>t', 'a ', 'ac', 'am', 'c ',\n",
       "        'ca', 'ch', 'e ', 'he', 'hi', 'i ', 'i<EOS>', 'm ', 'mi', 'ta',\n",
       "        'ti'], dtype='<U6'),\n",
       " array([1, 1, 2, 1, 5, 1, 1, 4, 1, 2, 1, 2, 1, 1, 1, 5, 1, 1, 1, 4, 2]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_string = \"ti che ti tachi i tac tacam i tac a mi\"\n",
    "bigram_counter(in_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(we don't worry about the order of bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's strip punctuation from the sentences, then tokenize all the sentences in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lang</th>\n",
       "      <th>sentence</th>\n",
       "      <th>clean_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ldn</td>\n",
       "      <td>Meháya rul woho wa.</td>\n",
       "      <td>Meháya rul woho wa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eng</td>\n",
       "      <td>I want to borrow some money.</td>\n",
       "      <td>I want to borrow some money</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bar</td>\n",
       "      <td>Ohne Schmäh?</td>\n",
       "      <td>Ohne Schmäh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>vol</td>\n",
       "      <td>Fat Tomasa äbinom sanel.</td>\n",
       "      <td>Fat Tomasa äbinom sanel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ldn</td>\n",
       "      <td>Bíid thad dóyom le leyóoth wa, Thom.</td>\n",
       "      <td>Bíid thad dóyom le leyóoth wa Thom</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  lang                              sentence  \\\n",
       "0  ldn                   Meháya rul woho wa.   \n",
       "1  eng          I want to borrow some money.   \n",
       "2  bar                          Ohne Schmäh?   \n",
       "3  vol              Fat Tomasa äbinom sanel.   \n",
       "4  ldn  Bíid thad dóyom le leyóoth wa, Thom.   \n",
       "\n",
       "                       clean_sentence  \n",
       "0                  Meháya rul woho wa  \n",
       "1         I want to borrow some money  \n",
       "2                         Ohne Schmäh  \n",
       "3             Fat Tomasa äbinom sanel  \n",
       "4  Bíid thad dóyom le leyóoth wa Thom  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_df.loc[:, \"clean_sentence\"] = dev_df.apply(lambda row : row.sentence.translate(str.maketrans('', '', string.punctuation)),\n",
    "                                               axis=1)\n",
    "dev_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_df.loc[:, \"tokenized\"] = dev_df.apply(lambda row : bigram_counter(row.clean_sentence)[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lang</th>\n",
       "      <th>sentence</th>\n",
       "      <th>clean_sentence</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ldn</td>\n",
       "      <td>Meháya rul woho wa.</td>\n",
       "      <td>Meháya rul woho wa</td>\n",
       "      <td>[ r,  w, &lt;BOS&gt;M, Me, a , a&lt;EOS&gt;, eh, ho, há, l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eng</td>\n",
       "      <td>I want to borrow some money.</td>\n",
       "      <td>I want to borrow some money</td>\n",
       "      <td>[ b,  m,  s,  t,  w, &lt;BOS&gt;I, I , an, bo, e , e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bar</td>\n",
       "      <td>Ohne Schmäh?</td>\n",
       "      <td>Ohne Schmäh</td>\n",
       "      <td>[ S, &lt;BOS&gt;O, Oh, Sc, ch, e , h&lt;EOS&gt;, hm, hn, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>vol</td>\n",
       "      <td>Fat Tomasa äbinom sanel.</td>\n",
       "      <td>Fat Tomasa äbinom sanel</td>\n",
       "      <td>[ T,  s,  ä, &lt;BOS&gt;F, Fa, To, a , an, as, at, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ldn</td>\n",
       "      <td>Bíid thad dóyom le leyóoth wa, Thom.</td>\n",
       "      <td>Bíid thad dóyom le leyóoth wa Thom</td>\n",
       "      <td>[ T,  d,  l,  t,  w, &lt;BOS&gt;B, Bí, Th, a , ad, d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  lang                              sentence  \\\n",
       "0  ldn                   Meháya rul woho wa.   \n",
       "1  eng          I want to borrow some money.   \n",
       "2  bar                          Ohne Schmäh?   \n",
       "3  vol              Fat Tomasa äbinom sanel.   \n",
       "4  ldn  Bíid thad dóyom le leyóoth wa, Thom.   \n",
       "\n",
       "                       clean_sentence  \\\n",
       "0                  Meháya rul woho wa   \n",
       "1         I want to borrow some money   \n",
       "2                         Ohne Schmäh   \n",
       "3             Fat Tomasa äbinom sanel   \n",
       "4  Bíid thad dóyom le leyóoth wa Thom   \n",
       "\n",
       "                                           tokenized  \n",
       "0  [ r,  w, <BOS>M, Me, a , a<EOS>, eh, ho, há, l...  \n",
       "1  [ b,  m,  s,  t,  w, <BOS>I, I , an, bo, e , e...  \n",
       "2  [ S, <BOS>O, Oh, Sc, ch, e , h<EOS>, hm, hn, m...  \n",
       "3  [ T,  s,  ä, <BOS>F, Fa, To, a , an, as, at, b...  \n",
       "4  [ T,  d,  l,  t,  w, <BOS>B, Bí, Th, a , ad, d...  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, count bigrams in each sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_df.loc[:, \"counts\"] = dev_df.apply(lambda row : bigram_counter(row.clean_sentence)[1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lang</th>\n",
       "      <th>sentence</th>\n",
       "      <th>clean_sentence</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ldn</td>\n",
       "      <td>Meháya rul woho wa.</td>\n",
       "      <td>Meháya rul woho wa</td>\n",
       "      <td>[ r,  w, &lt;BOS&gt;M, Me, a , a&lt;EOS&gt;, eh, ho, há, l...</td>\n",
       "      <td>[1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eng</td>\n",
       "      <td>I want to borrow some money.</td>\n",
       "      <td>I want to borrow some money</td>\n",
       "      <td>[ b,  m,  s,  t,  w, &lt;BOS&gt;I, I , an, bo, e , e...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bar</td>\n",
       "      <td>Ohne Schmäh?</td>\n",
       "      <td>Ohne Schmäh</td>\n",
       "      <td>[ S, &lt;BOS&gt;O, Oh, Sc, ch, e , h&lt;EOS&gt;, hm, hn, m...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>vol</td>\n",
       "      <td>Fat Tomasa äbinom sanel.</td>\n",
       "      <td>Fat Tomasa äbinom sanel</td>\n",
       "      <td>[ T,  s,  ä, &lt;BOS&gt;F, Fa, To, a , an, as, at, b...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ldn</td>\n",
       "      <td>Bíid thad dóyom le leyóoth wa, Thom.</td>\n",
       "      <td>Bíid thad dóyom le leyóoth wa Thom</td>\n",
       "      <td>[ T,  d,  l,  t,  w, &lt;BOS&gt;B, Bí, Th, a , ad, d...</td>\n",
       "      <td>[1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  lang                              sentence  \\\n",
       "0  ldn                   Meháya rul woho wa.   \n",
       "1  eng          I want to borrow some money.   \n",
       "2  bar                          Ohne Schmäh?   \n",
       "3  vol              Fat Tomasa äbinom sanel.   \n",
       "4  ldn  Bíid thad dóyom le leyóoth wa, Thom.   \n",
       "\n",
       "                       clean_sentence  \\\n",
       "0                  Meháya rul woho wa   \n",
       "1         I want to borrow some money   \n",
       "2                         Ohne Schmäh   \n",
       "3             Fat Tomasa äbinom sanel   \n",
       "4  Bíid thad dóyom le leyóoth wa Thom   \n",
       "\n",
       "                                           tokenized  \\\n",
       "0  [ r,  w, <BOS>M, Me, a , a<EOS>, eh, ho, há, l...   \n",
       "1  [ b,  m,  s,  t,  w, <BOS>I, I , an, bo, e , e...   \n",
       "2  [ S, <BOS>O, Oh, Sc, ch, e , h<EOS>, hm, hn, m...   \n",
       "3  [ T,  s,  ä, <BOS>F, Fa, To, a , an, as, at, b...   \n",
       "4  [ T,  d,  l,  t,  w, <BOS>B, Bí, Th, a , ad, d...   \n",
       "\n",
       "                                              counts  \n",
       "0  [1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "2               [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  \n",
       "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "4  [1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, ...  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's collect the \"vocabulary\" of bigrams of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = np.unique([bigram for tknzd_sentence in dev_df[\"tokenized\"] for bigram in tknzd_sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7011"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we want to initialize a sparse matrix using `scipy` with as many rows as the number of sentences in the dataset and as many columns as the number of bigrams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import dok_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_matrix = dok_matrix((dev_df.shape[0], len(vocab)), dtype = np.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(dev_matrix.shape[0]): # for each sentence\n",
    "    for j in range(dev_matrix.shape[1]): # for each bigram in the vocabulary\n",
    "        if vocab[j] in dev_df[\"tokenized\"][i]: # if the bigram is found in the current sentence...\n",
    "            dev_matrix[i, j] = dev_df[\"counts\"][i][dev_df[\"tokenized\"][i] == vocab[j]] # ...the value of the matrix is the count of that bigram in that sentence...\n",
    "        else:\n",
    "            dev_matrix[i, j] = 0 # ...otherwise is 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
