{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TueSNLP - Assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language identification\n",
    "The assignment is available here: https://snlp2018.github.io/assignments.html."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "This exercise is about creating a dataset of sentences in different languages starting from ids of tweets collected during the class. We don't have access to the private repo where the tweets had been saved, so we try to keep the spirit of the exercise using data from http://tatoeba.org a crowd-sourced collection of sentences and translations.\n",
    "\n",
    "In particular, http://downloads.tatoeba.org/exports/sentences.tar.bz2 contains ~8 milion sentences each with corresponding language code. In order to mimic the dataset originally provided for the assignment, we select 30 languages at random and pick a certain number of random sentences for each language. We use the majority of sentences to build the development set and the remainder to build an evaluation set which will be used to evaluate our model(s). Precise quantities and ratio will be determined after we explore the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import progressbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>lang</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>cmn</td>\n",
       "      <td>我們試試看！</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>cmn</td>\n",
       "      <td>我该去睡觉了。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>cmn</td>\n",
       "      <td>你在干什麼啊？</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>cmn</td>\n",
       "      <td>這是什麼啊？</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>cmn</td>\n",
       "      <td>今天是６月１８号，也是Muiriel的生日！</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id lang                sentence\n",
       "0   1  cmn                  我們試試看！\n",
       "1   2  cmn                 我该去睡觉了。\n",
       "2   3  cmn                 你在干什麼啊？\n",
       "3   4  cmn                  這是什麼啊？\n",
       "4   5  cmn  今天是６月１８号，也是Muiriel的生日！"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read data\n",
    "full_data_raw = pd.read_csv(\"data/sentences.csv\", sep = \"\\t\", names = [\"id\", \"lang\", \"sentence\"])\n",
    "full_data_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't need sentence id, we can drop the column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data_raw = full_data_raw.drop(\"id\", axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lang            349\n",
       "sentence    8112291\n",
       "dtype: int64"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many unique languages?\n",
    "full_data_raw.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pick 30 languages at random, then filter the df:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wln', 'ban', 'ary', 'mah', 'vro', 'kaa', 'mwl', 'dsb', 'tel', 'kam', 'hrx', 'dws', 'ldn', 'urh', 'tlh', 'pol', 'lmo', 'tsn', 'mnw', 'enm', '\\\\N', 'lou', 'kha', 'mhr', 'prg', 'arz', 'dng', 'srd', 'nav', 'cor']\n"
     ]
    }
   ],
   "source": [
    "random.seed(2)\n",
    "languages = random.sample(set(full_data_raw[\"lang\"]), 30)\n",
    "print(languages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `\\\\N` is suspicious, let's see what it corresponds to in the df:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lang</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6095954</th>\n",
       "      <td>\\N</td>\n",
       "      <td>Sābuku mamayamin, niyaꞋ takiteku manaꞋul magla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6104067</th>\n",
       "      <td>\\N</td>\n",
       "      <td>Kataau kano koson i Ama' min pana mataau!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6310672</th>\n",
       "      <td>\\N</td>\n",
       "      <td>Нуӈан дэмэрипчут дылви амаскиви донӈорочон.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6310678</th>\n",
       "      <td>\\N</td>\n",
       "      <td>Чикчакун дочадяран, мудана ачинди иргикэндиви ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6310683</th>\n",
       "      <td>\\N</td>\n",
       "      <td>Том сома дэмэр куӈакан бичэн.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        lang                                           sentence\n",
       "6095954   \\N  Sābuku mamayamin, niyaꞋ takiteku manaꞋul magla...\n",
       "6104067   \\N          Kataau kano koson i Ama' min pana mataau!\n",
       "6310672   \\N        Нуӈан дэмэрипчут дылви амаскиви донӈорочон.\n",
       "6310678   \\N  Чикчакун дочадяран, мудана ачинди иргикэндиви ...\n",
       "6310683   \\N                      Том сома дэмэр куӈакан бичэн."
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter df\n",
    "full_data_raw[full_data_raw[\"lang\"] == \"\\\\N\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears to be a placeholder for missing language codes. Let's filter out the corresponding rows:\n",
    "\n",
    "(eventually, if our classifier work well enough, we will be able to use it to infer the missing language codes...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lang            348\n",
       "sentence    8112201\n",
       "dtype: int64"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_data = full_data_raw[full_data_raw[\"lang\"] != \"\\\\N\"]\n",
    "filtered_data.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't know how many sentences there are for each language. Before sampling 30 languages, let's remove those with less than 100 sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lang</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lang</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>abk</th>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>acm</th>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ady</th>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>afb</th>\n",
       "      <td>1</td>\n",
       "      <td>133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>afh</th>\n",
       "      <td>1</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      lang  sentence\n",
       "lang                \n",
       "abk      1        26\n",
       "acm      1        49\n",
       "ady      1        31\n",
       "afb      1       133\n",
       "afh      1        79"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_count = filtered_data.groupby(\"lang\").nunique() # group by language and count sentences per group\n",
    "sentence_count.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "178"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract only languages with >99 sentences\n",
    "languages = sentence_count[sentence_count[\"sentence\"] > 99].index.values\n",
    "len(languages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Among these, we sample 30:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bar', 'cbk', 'bul', 'ldn', 'eus', 'wuu', 'kab', 'hrx', 'tha', 'got', 'vol', 'ast', 'swe', 'eng', 'nds', 'mar', 'prg', 'lit', 'sah', 'nob', 'pol', 'ido', 'urd', 'arz', 'lfn', 'ori', 'kaz', 'lvs', 'mya', 'rom']\n"
     ]
    }
   ],
   "source": [
    "random.seed(2)\n",
    "languages = random.sample(list(languages), 30)\n",
    "print(languages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's pick 50 sentences at random for each language:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lang</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>arz</td>\n",
       "      <td>كل الناس بتحب المكان ده.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>arz</td>\n",
       "      <td>كله محصل بعضه.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>arz</td>\n",
       "      <td>كارلوس طلع الجبل.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>arz</td>\n",
       "      <td>مفيش أي مشاكل.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>arz</td>\n",
       "      <td>معدش بيشوف.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  lang                  sentence\n",
       "0  arz  كل الناس بتحب المكان ده.\n",
       "1  arz            كله محصل بعضه.\n",
       "2  arz         كارلوس طلع الجبل.\n",
       "3  arz            مفيش أي مشاكل.\n",
       "4  arz               معدش بيشوف."
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_data = filtered_data[filtered_data[\"lang\"].isin(languages)] # filter based on list of sampled languages\n",
    "# we apply a sampling function groupwise\n",
    "sampled_data = sampled_data.groupby(\"lang\").apply(lambda x : x.sample(50, random_state = 2)).reset_index(drop=True)\n",
    "sampled_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1500"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's shuffle this and save as tsv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lang</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lit</td>\n",
       "      <td>Ačiū, kad tu atėjai manęs gelbėti.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vol</td>\n",
       "      <td>Ävilob jonön omes buki olik.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>swe</td>\n",
       "      <td>De sade att de skulle göra läxor, men i ställe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ast</td>\n",
       "      <td>Tien la zuna de escargatiar nes ñarres.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>eng</td>\n",
       "      <td>You had a week to do this.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  lang                                           sentence\n",
       "0  lit                 Ačiū, kad tu atėjai manęs gelbėti.\n",
       "1  vol                       Ävilob jonön omes buki olik.\n",
       "2  swe  De sade att de skulle göra läxor, men i ställe...\n",
       "3  ast            Tien la zuna de escargatiar nes ñarres.\n",
       "4  eng                         You had a week to do this."
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_data = sampled_data.sample(n = sampled_data.shape[0], random_state = 2).reset_index(drop=True)\n",
    "sampled_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write to disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_data.to_csv(\"data/assignment3-data.tsv\", sep = \"\\t\", header = False, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This exercise is about feature extraction. Each sentence is tokenized at the level of character bigrams. Then each sentence is represented as an array of counts of bigrams, for each bigram in the dataset (so a very sparse array, the vast majority of entries will be 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to extract character-level bigrams\n",
    "def sentence_tokenizer(in_string): # takes a string as input\n",
    "    tknzd_string = [] # initialize empty output\n",
    "    for i in range(0, len(in_string) - 1):\n",
    "        tknzd_string.append(in_string[i] + in_string[i+1]) # each bigram is a character followed by the next one\n",
    "    tknzd_string.insert(0, \"<BOS>\" + tknzd_string[0][0]) # first bigram is always <BOS>+first character\n",
    "    tknzd_string.append(tknzd_string[-1][-1] + \"<EOS>\") # last bigram is always last character+<EOS>\n",
    "    return(tknzd_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<BOS>t', 'ti', 'i ', ' c', 'ch', 'he', 'e ', ' t', 'ti', 'i ', ' t', 'ta', 'ac', 'ch', 'hi', 'i ', ' i', 'i ', ' t', 'ta', 'ac', 'c ', ' t', 'ta', 'ac', 'ca', 'am', 'm ', ' i', 'i ', ' t', 'ta', 'ac', 'c ', ' a', 'a ', ' m', 'mi', 'i<EOS>']\n"
     ]
    }
   ],
   "source": [
    "in_string = \"ti che ti tachi i tac tacam i tac a mi\"\n",
    "out_string = sentence_tokenizer(in_string)\n",
    "print(out_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to count bigrams in a sentence\n",
    "def bigram_counter(in_string):\n",
    "    tknzd_string = sentence_tokenizer(in_string)\n",
    "    bigrams, counts = np.unique(tknzd_string, return_counts=True)\n",
    "    return(bigrams, counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([' a', ' c', ' i', ' m', ' t', '<BOS>t', 'a ', 'ac', 'am', 'c ',\n",
       "        'ca', 'ch', 'e ', 'he', 'hi', 'i ', 'i<EOS>', 'm ', 'mi', 'ta',\n",
       "        'ti'], dtype='<U6'),\n",
       " array([1, 1, 2, 1, 5, 1, 1, 4, 1, 2, 1, 2, 1, 1, 1, 5, 1, 1, 1, 4, 2]))"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_string = \"ti che ti tachi i tac tacam i tac a mi\"\n",
    "bigram_counter(in_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(we don't worry about the order of bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's strip punctuation from the sentences, then tokenize all the sentences in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lang</th>\n",
       "      <th>sentence</th>\n",
       "      <th>clean_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lit</td>\n",
       "      <td>Ačiū, kad tu atėjai manęs gelbėti.</td>\n",
       "      <td>Ačiū kad tu atėjai manęs gelbėti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vol</td>\n",
       "      <td>Ävilob jonön omes buki olik.</td>\n",
       "      <td>Ävilob jonön omes buki olik</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>swe</td>\n",
       "      <td>De sade att de skulle göra läxor, men i ställe...</td>\n",
       "      <td>De sade att de skulle göra läxor men i stället...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ast</td>\n",
       "      <td>Tien la zuna de escargatiar nes ñarres.</td>\n",
       "      <td>Tien la zuna de escargatiar nes ñarres</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>eng</td>\n",
       "      <td>You had a week to do this.</td>\n",
       "      <td>You had a week to do this</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  lang                                           sentence  \\\n",
       "0  lit                 Ačiū, kad tu atėjai manęs gelbėti.   \n",
       "1  vol                       Ävilob jonön omes buki olik.   \n",
       "2  swe  De sade att de skulle göra läxor, men i ställe...   \n",
       "3  ast            Tien la zuna de escargatiar nes ñarres.   \n",
       "4  eng                         You had a week to do this.   \n",
       "\n",
       "                                      clean_sentence  \n",
       "0                   Ačiū kad tu atėjai manęs gelbėti  \n",
       "1                        Ävilob jonön omes buki olik  \n",
       "2  De sade att de skulle göra läxor men i stället...  \n",
       "3             Tien la zuna de escargatiar nes ñarres  \n",
       "4                          You had a week to do this  "
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_data.loc[:, \"clean_sentence\"] = sampled_data.apply(lambda row : row.sentence.translate(str.maketrans('', '', string.punctuation)),\n",
    "                                               axis=1)\n",
    "sampled_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_data.loc[:, \"tokenized\"] = sampled_data.apply(lambda row : bigram_counter(row.clean_sentence)[0], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, count bigrams in each sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_data.loc[:, \"counts\"] = sampled_data.apply(lambda row : bigram_counter(row.clean_sentence)[1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lang</th>\n",
       "      <th>sentence</th>\n",
       "      <th>clean_sentence</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lit</td>\n",
       "      <td>Ačiū, kad tu atėjai manęs gelbėti.</td>\n",
       "      <td>Ačiū kad tu atėjai manęs gelbėti</td>\n",
       "      <td>[ a,  g,  k,  m,  t, &lt;BOS&gt;A, Ač, ad, ai, an, a...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vol</td>\n",
       "      <td>Ävilob jonön omes buki olik.</td>\n",
       "      <td>Ävilob jonön omes buki olik</td>\n",
       "      <td>[ b,  j,  o, &lt;BOS&gt;Ä, b , bu, es, i , ik, il, j...</td>\n",
       "      <td>[1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>swe</td>\n",
       "      <td>De sade att de skulle göra läxor, men i ställe...</td>\n",
       "      <td>De sade att de skulle göra läxor men i stället...</td>\n",
       "      <td>[ a,  b,  d,  g,  i,  l,  m,  p,  s, &lt;BOS&gt;D, D...</td>\n",
       "      <td>[1, 1, 2, 2, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ast</td>\n",
       "      <td>Tien la zuna de escargatiar nes ñarres.</td>\n",
       "      <td>Tien la zuna de escargatiar nes ñarres</td>\n",
       "      <td>[ d,  e,  l,  n,  z,  ñ, &lt;BOS&gt;T, Ti, a , ar, a...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 2, 3, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>eng</td>\n",
       "      <td>You had a week to do this.</td>\n",
       "      <td>You had a week to do this</td>\n",
       "      <td>[ a,  d,  h,  t,  w, &lt;BOS&gt;Y, Yo, a , ad, d , d...</td>\n",
       "      <td>[1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  lang                                           sentence  \\\n",
       "0  lit                 Ačiū, kad tu atėjai manęs gelbėti.   \n",
       "1  vol                       Ävilob jonön omes buki olik.   \n",
       "2  swe  De sade att de skulle göra läxor, men i ställe...   \n",
       "3  ast            Tien la zuna de escargatiar nes ñarres.   \n",
       "4  eng                         You had a week to do this.   \n",
       "\n",
       "                                      clean_sentence  \\\n",
       "0                   Ačiū kad tu atėjai manęs gelbėti   \n",
       "1                        Ävilob jonön omes buki olik   \n",
       "2  De sade att de skulle göra läxor men i stället...   \n",
       "3             Tien la zuna de escargatiar nes ñarres   \n",
       "4                          You had a week to do this   \n",
       "\n",
       "                                           tokenized  \\\n",
       "0  [ a,  g,  k,  m,  t, <BOS>A, Ač, ad, ai, an, a...   \n",
       "1  [ b,  j,  o, <BOS>Ä, b , bu, es, i , ik, il, j...   \n",
       "2  [ a,  b,  d,  g,  i,  l,  m,  p,  s, <BOS>D, D...   \n",
       "3  [ d,  e,  l,  n,  z,  ñ, <BOS>T, Ti, a , ar, a...   \n",
       "4  [ a,  d,  h,  t,  w, <BOS>Y, Yo, a , ad, d , d...   \n",
       "\n",
       "                                              counts  \n",
       "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "1  [1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "2  [1, 1, 2, 2, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, ...  \n",
       "3  [1, 1, 1, 1, 1, 1, 1, 1, 2, 3, 1, 1, 1, 1, 1, ...  \n",
       "4  [1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  "
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's collect the \"vocabulary\" of bigrams of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = np.unique([bigram for tknzd_sentence in sampled_data[\"tokenized\"] for bigram in tknzd_sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5485"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the complete vocabulary, we can split the dataset into development (90%) and evaluation (10%) set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_df = sampled_data.head(int(sampled_data.shape[0]*9/10))\n",
    "eval_df = sampled_data.tail(int(sampled_data.shape[0]*1/10)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each set we want to initialize (then fill) a sparse matrix using `scipy` with as many rows as the number of sentences in the development or evaluationset and as many columns as the number of bigrams in the full vocab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import dok_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_count_vectorizer(df, vocab): # df must have a 'lang' column \n",
    "    matrix = dok_matrix((df.shape[0], len(vocab)), dtype = np.int16) # initialize matrix\n",
    "    \n",
    "    # it might take a while, so let's add a progressbar to have an idea of the running time\n",
    "    bar = progressbar.ProgressBar(maxval = matrix.shape[0], \\\n",
    "        widgets = [progressbar.Bar('=', '[', ']'), ' ', progressbar.Percentage()])\n",
    "\n",
    "    bar.start()\n",
    "    for i in range(matrix.shape[0]): # for each sentence,\n",
    "        for j in range(matrix.shape[1]): # for each bigram in the vocabulary,\n",
    "            if vocab[j] in dev_df[\"tokenized\"][i]: # if the bigram is found in the current sentence...\n",
    "                matrix[i, j] = dev_df[\"counts\"][i][dev_df[\"tokenized\"][i] == vocab[j]] # ...the value of the matrix is the count of that bigram in that sentence...\n",
    "            else:\n",
    "                matrix[i, j] = 0 # ...otherwise is 0\n",
    "        bar.update(i+1)\n",
    "    bar.finish()\n",
    "    \n",
    "    return(matrix, df['lang'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's fill the matrix for the development set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[========================================================================] 100%\n"
     ]
    }
   ],
   "source": [
    "dev_matrix, dev_tags = data_count_vectorizer(dev_df, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "Logistic regression using `sklearn.linear_model.LogisticRegression`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model as lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "logReg = lm.LogisticRegression() # initialize model\n",
    "\n",
    "X_train = dev_matrix # counts of bigrams as features\n",
    "y_train = dev_tags # language label as target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logReg.fit(X_train, y_train) # train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logReg.score(X_train, y_train) # print score (mean accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about score on evaluation set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[========================================================================] 100%\n"
     ]
    }
   ],
   "source": [
    "eval_matrix, eval_tags = data_count_vectorizer(eval_df, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03333333333333333"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_eval = eval_matrix\n",
    "y_eval = eval_tags\n",
    "logReg.score(X_eval, y_eval) # print score (mean accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty bad generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "We write a function that takes two sequences of labels (read: gold-standard labels and predicted labels) and a specific label as input and returns precision, recall and F1-score with respect to the input specific label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(true_labels, predicted_labels, label):\n",
    "    true_indices = [i for i, x in enumerate(true_labels) if x == label] # indices of true occurrences of \"label\"\n",
    "    predicted_indices = [i for i, x in enumerate(predicted_labels) if x == label] # predictions of true occ. of \"label\"\n",
    "    neg_true_indices = [i for i, x in enumerate(true_labels) if x != label] # indices of true occurrences of \"NOTlabel\"\n",
    "    neg_predicted_indices = [i for i, x in enumerate(predicted_labels) if x != label] # predictions of true occ. of \"NOTlabel\"\n",
    "\n",
    "    tp_indices = list(set(true_indices).intersection(predicted_indices)) # indices of true positives\n",
    "    fp_indices = [i for i in predicted_indices if not i in tp_indices] # indices of false positives\n",
    "    tn_indices = list(set(neg_true_indices).intersection(neg_predicted_indices)) # indices of true negatives\n",
    "    fn_indices = [i for i in neg_predicted_indices if not i in tn_indices] # indices of false negatives\n",
    "\n",
    "    tp = len(tp_indices) # how many?\n",
    "    fp = len(fp_indices)\n",
    "    fn = len(fn_indices)\n",
    "    \n",
    "    ###TODO: add check for division by zero :)\n",
    "    precision = tp/(tp+fp) # scores\n",
    "    recall = tp/(tp+fn)\n",
    "    f1_score = (2*precision*recall)/(precision+recall)\n",
    "\n",
    "    return(precision, recall, f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = [\"ita\", \"fra\", \"eng\", \"deu\", \"esp\", \"por\", \"swe\", \n",
    "               \"ita\", \"fra\", \"eng\", \"deu\", \"esp\", \"por\", \"swe\",\n",
    "               \"ita\", \"fra\", \"eng\", \"deu\", \"esp\", \"por\", \"swe\", \n",
    "               \"ita\", \"fra\", \"eng\", \"deu\", \"esp\", \"por\", \"swe\", \n",
    "               \"ita\", \"fra\", \"eng\", \"deu\", \"esp\", \"por\", \"swe\",\n",
    "               \"ita\", \"fra\", \"eng\", \"deu\", \"esp\", \"por\", \"swe\",\n",
    "               \"ita\", \"fra\", \"eng\", \"deu\", \"esp\", \"por\", \"swe\"]\n",
    "\n",
    "predicted_labels = [\"ita\", \"fra\", \"fra\", \"eng\", \"por\", \"por\", \"esp\", \n",
    "                    \"ita\", \"fra\", \"deu\", \"deu\", \"esp\", \"por\", \"swe\",\n",
    "                    \"ita\", \"fra\", \"deu\", \"deu\", \"por\", \"por\", \"swe\", \n",
    "                    \"ita\", \"fra\", \"deu\", \"deu\", \"esp\", \"por\", \"swe\", \n",
    "                    \"ita\", \"fra\", \"deu\", \"deu\", \"esp\", \"por\", \"swe\",\n",
    "                    \"ita\", \"fra\", \"deu\", \"deu\", \"por\", \"esp\", \"swe\",\n",
    "                    \"ita\", \"fra\", \"eng\", \"swe\", \"esp\", \"por\", \"swe\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 1.0, Recall: 1.0, F1-score: 1.0\n"
     ]
    }
   ],
   "source": [
    "pr, re, f1 = get_scores(true_labels, predicted_labels, \"ita\")\n",
    "print(\"Precision: {}, Recall: {}, F1-score: {}\".format(pr, re, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 1.0, Recall: 0.5714285714285714, F1-score: 0.7272727272727273\n"
     ]
    }
   ],
   "source": [
    "pr, re, f1 = get_scores(true_labels, predicted_labels, \"fra\")\n",
    "print(\"Precision: {}, Recall: {}, F1-score: {}\".format(pr, re, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.8571428571428571, Recall: 0.8571428571428571, F1-score: 0.8571428571428571\n"
     ]
    }
   ],
   "source": [
    "pr, re, f1 = get_scores(true_labels, predicted_labels, \"swe\")\n",
    "print(\"Precision: {}, Recall: {}, F1-score: {}\".format(pr, re, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we write a function to compute averaged scores over a list of specific labels (instead of just one label as before):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_scores(true_labels, predicted_labels, labels):\n",
    "    precisions = [] # initialize empty lists\n",
    "    recalls = []\n",
    "    f1_scores = []\n",
    "    for label in list(set(labels)): # cycle through unique entries in label list\n",
    "        pr, re, f1 = get_scores(true_labels, predicted_labels, label)\n",
    "        precisions.append(pr)\n",
    "        recalls.append(re)\n",
    "        f1_scores.append(f1)\n",
    "    avg_precision = sum(precisions)/len(precisions) # compute average scores\n",
    "    avg_recall = sum(recalls)/len(recalls)\n",
    "    avg_f1_score = sum(f1_scores)/len(f1_scores)\n",
    "    \n",
    "    return(avg_precision, avg_recall, avg_f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average scores. Precision: 0.7414965986394557, Recall: 0.7346938775510203, F1-score: 0.7282108647654866\n"
     ]
    }
   ],
   "source": [
    "labels = [\"ita\", \"fra\", \"eng\", \"deu\", \"esp\", \"por\", \"swe\"]\n",
    "avg_pr, avg_re, avg_f1 = avg_scores(true_labels, predicted_labels, labels)\n",
    "print(\"Average scores. Precision: {}, Recall: {}, F1-score: {}\".format(avg_pr, avg_re, avg_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average scores. Precision: 0.9375, Recall: 1.0, F1-score: 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "labels = [\"ita\", \"fra\"]\n",
    "avg_pr, avg_re, avg_f1 = avg_scores(true_labels, predicted_labels, labels)\n",
    "print(\"Average scores. Precision: {}, Recall: {}, F1-score: {}\".format(avg_pr, avg_re, avg_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's copmute these average scores for the model trained above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = dev_tags\n",
    "predicted_labels = logReg.predict(X_train)\n",
    "labels = list(set(dev_df['lang']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average scores. Precision: 1.0, Recall: 1.0, F1-score: 1.0\n"
     ]
    }
   ],
   "source": [
    "avg_pr, avg_re, avg_f1 = avg_scores(true_labels, predicted_labels, labels)\n",
    "print(\"Average scores. Precision: {}, Recall: {}, F1-score: {}\".format(avg_pr, avg_re, avg_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double check with `sklearn.metrics`?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
