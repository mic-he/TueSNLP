{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37664bitbasevenv23cd022687ab470688da671eade16340",
   "display_name": "Python 3.7.6 64-bit ('base': venv)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 6\n",
    "## Language model\n",
    "The assignment and data are available here: https://snlp2018.github.io/assignments.html\n",
    "\n",
    "The `train` folder in the archive contains several text file produced by students of the course as self-introduction; the `test` folder contains a few text files of the same kind."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excercise 1\n",
    "Tokenization. Using `nltk`, we write a function which takes a variable number of text files as input and returns a tokenize corpus (list of lists of tokens) and the corpus' vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import wordpunct_tokenize\n",
    "from glob import glob\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"data/train/\" # this is the argument of the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(path_to_files):\n",
    "    input_files = glob(input_path + \"*.txt\") # get all the txt files in the folder\n",
    "\n",
    "    texts = [] # initialize empty list where we collect data\n",
    "    for input_file in input_files:\n",
    "        with open(input_file, \"r\", encoding = \"utf-8\") as in_file: # read the file\n",
    "            text = in_file.read().split(\".\") # split sentences\n",
    "            for line in text:\n",
    "                tknzd_line = wordpunct_tokenize(line.lower()) # lowercase and tokenize each sentence\n",
    "                if tknzd_line != []:\n",
    "                    tknzd_line.insert(0, \"<s>\") # add beginning-of-sentence,\n",
    "                    tknzd_line.insert(0, \"<s>\") # (twice because we'll be dealing with 3-grams)\n",
    "                    tknzd_line.append(\"</s>\") # and end-of-sentence tags\n",
    "                    texts.append(tknzd_line)\n",
    "    \n",
    "    tokens = [token for sentence in texts for token in sentence]\n",
    "    vocab = sorted(set(tokens)) # list of unique tokens\n",
    "\n",
    "    return(texts, tokens, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus, all_tokens, vocab = tokenize(input_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[['<s>', '<s>', 'i', 'always', 'wanted', 'to', 'work', 'with', 'words', 'or', 'language', ',', 'and', 'it', 'didn', '’', 't', 'matter', 'for', 'me', 'would', 'it', 'be', 'something', 'related', 'to', 'neurolinguistics', ',', 'just', 'a', 'simple', 'translation', 'work', 'or', 'anything', 'more', 'computational', '</s>']]\n"
    }
   ],
   "source": [
    "print(corpus[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "['!', '!“', '\"', '#', \"'\", '(', ')', '),', ')?', ',', '-', '/', '/?', '1m', '1st', '2000', '2004', '2011', '2016', '20s', '3', ':', '://', ';', '</s>', '<s>', '=', '>', '?', '?\",', '@', 'a', 'ability', 'able', 'about', 'above', 'abovementioned', 'access', 'achieved', 'achievement', 'acquainted', 'acquire', 'acquiring', 'acquisition', 'across', 'activity', 'actually', 'addictive', 'addition', 'addresses', 'advantaged', 'after', 'age', 'agricultural', 'ai', 'aid', 'aided', 'alexa', 'algorithms', 'aligned', 'all', 'allow', 'allowing', 'allows', 'almost', 'alone', 'along', 'already', 'also', 'alter', 'although', 'always', 'am', 'amazigh', 'amazing', 'amazon', 'america', 'among', 'amount', 'an', 'anagrams', 'analyse', 'analysing', 'analysis', 'analytics', 'analyze', 'analyzing', 'and', 'another', 'answer', 'any', 'anymore', 'anyone', 'anything', 'apart', 'api', 'appealing', 'application', 'applications', 'apply']\n"
    }
   ],
   "source": [
    "print(vocab[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "5841"
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shall we remove punctuation as pre-processing? We'll see..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "1192"
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excercise 2\n",
    "N-gram language model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, write a function to extract n-grams from a tokenized sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_sentence(n, sentence): # very basic, accepts only n=1, n=2, n=3\n",
    "    if n == 1:\n",
    "        out_sentence = [(token) for token in sentence]\n",
    "    elif n == 2:\n",
    "        out_sentence = [(sentence[i], sentence[i+1]) for i in range(0, len(sentence) - (n-1))]\n",
    "    else:\n",
    "        out_sentence = [(sentence[i], sentence[i+1], sentence[i+2]) for i in range(0, len(sentence) - (n-1))]\n",
    "    return(out_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "['<s>', '<s>', 'in', 'the', 'end', 'i', 'got', 'a', 'bit', 'more', 'interested', 'in', 'computer', 'linguistics', ',', 'because', 'in', 'school', 'i', 'always', 'was', 'good', 'with', 'programming', 'and', 'it', 'felt', 'natural', 'to', 'continue', 'improving', 'myself', 'in', 'that', 'field', '</s>']\n"
    }
   ],
   "source": [
    "print(ngram_sentence(1, sentence = corpus[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[('<s>', '<s>', 'in'), ('<s>', 'in', 'the'), ('in', 'the', 'end'), ('the', 'end', 'i'), ('end', 'i', 'got'), ('i', 'got', 'a'), ('got', 'a', 'bit'), ('a', 'bit', 'more'), ('bit', 'more', 'interested'), ('more', 'interested', 'in'), ('interested', 'in', 'computer'), ('in', 'computer', 'linguistics'), ('computer', 'linguistics', ','), ('linguistics', ',', 'because'), (',', 'because', 'in'), ('because', 'in', 'school'), ('in', 'school', 'i'), ('school', 'i', 'always'), ('i', 'always', 'was'), ('always', 'was', 'good'), ('was', 'good', 'with'), ('good', 'with', 'programming'), ('with', 'programming', 'and'), ('programming', 'and', 'it'), ('and', 'it', 'felt'), ('it', 'felt', 'natural'), ('felt', 'natural', 'to'), ('natural', 'to', 'continue'), ('to', 'continue', 'improving'), ('continue', 'improving', 'myself'), ('improving', 'myself', 'in'), ('myself', 'in', 'that'), ('in', 'that', 'field'), ('that', 'field', '</s>')]\n"
    }
   ],
   "source": [
    "print(ngram_sentence(3, sentence = corpus[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we write a function which counts the occurrences of each unique n-gram in a given sentence and updates a dictionary with the counts; the keys of the dictionary are the unique n-grams found in the sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_update(unique_ngrams, sentence):\n",
    "    for ngram in sentence:\n",
    "        if ngram in unique_ngrams.keys():\n",
    "            unique_ngrams[ngram] += 1\n",
    "        else:\n",
    "            unique_ngrams[ngram] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_ngrams = {} # initialize empty dict\n",
    "ngram_update(example_ngrams, ngram_sentence(3, sentence = corpus[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "{('<s>', '<s>', 'i'): 1, ('<s>', 'i', 'always'): 1, ('i', 'always', 'wanted'): 1, ('always', 'wanted', 'to'): 1, ('wanted', 'to', 'work'): 1, ('to', 'work', 'with'): 1, ('work', 'with', 'words'): 1, ('with', 'words', 'or'): 1, ('words', 'or', 'language'): 1, ('or', 'language', ','): 1, ('language', ',', 'and'): 1, (',', 'and', 'it'): 1, ('and', 'it', 'didn'): 1, ('it', 'didn', '’'): 1, ('didn', '’', 't'): 1, ('’', 't', 'matter'): 1, ('t', 'matter', 'for'): 1, ('matter', 'for', 'me'): 1, ('for', 'me', 'would'): 1, ('me', 'would', 'it'): 1, ('would', 'it', 'be'): 1, ('it', 'be', 'something'): 1, ('be', 'something', 'related'): 1, ('something', 'related', 'to'): 1, ('related', 'to', 'neurolinguistics'): 1, ('to', 'neurolinguistics', ','): 1, ('neurolinguistics', ',', 'just'): 1, (',', 'just', 'a'): 1, ('just', 'a', 'simple'): 1, ('a', 'simple', 'translation'): 1, ('simple', 'translation', 'work'): 1, ('translation', 'work', 'or'): 1, ('work', 'or', 'anything'): 1, ('or', 'anything', 'more'): 1, ('anything', 'more', 'computational'): 1, ('more', 'computational', '</s>'): 1}\n"
    }
   ],
   "source": [
    "print(example_ngrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply this function to our corpus and get 1-gram, 2-gram and 3-gram counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_1grams = {}\n",
    "for sentence in corpus:\n",
    "    ngram_update(counts_1grams, ngram_sentence(1, sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "{'<s>': 478, 'i': 164, 'always': 11, 'wanted': 4, 'to': 169, 'work': 14, 'with': 39, 'words': 7, 'or': 16, 'language': 55, ',': 198, 'and': 169, 'it': 63, 'didn': 3, '’': 4, 't': 5, 'matter': 1, 'for': 40, 'me': 26, 'would': 15, 'be': 27, 'something': 7, 'related': 3, 'neurolinguistics': 1, 'just': 6, 'a': 127, 'simple': 3, 'translation': 7, 'anything': 1, 'more': 27, 'computational': 45, '</s>': 239, 'in': 153, 'the': 194, 'end': 2, 'got': 4, 'bit': 3, 'interested': 28, 'computer': 21, 'linguistics': 67, 'because': 12, 'school': 3, 'was': 24, 'good': 1, 'programming': 17, 'felt': 1, 'natural': 10, 'continue': 1, 'improving': 3, 'myself': 2, 'that': 49, 'field': 16, 'also': 32, 'allow': 3, 'solve': 1, 'bigger': 1, 'variety': 2, 'of': 129, 'tasks': 5, ':': 3, 's': 13, 'really': 8, 'impressive': 1, 'how': 37, 'studying': 9, 'one': 17, 'thing': 6, 'can': 32, 'help': 6, 'you': 1, '“': 3, 'get': 2, 'access': 1, '”': 2, 'so': 17, 'many': 8, 'completely': 1, 'different': 12, 'fields': 5, 'starting': 3, 'from': 17, 'speech': 9, 'recognition': 5, 'machine': 19, 'another': 7, 'linguistic': 9, 'which': 20, 'find': 13, 'quite': 3, 'exciting': 5, 'is': 77, 'contrastive': 2, 'mean': 1, 'very': 21, 'interesting': 19, 'compare': 2, 'morphology': 1, 'syntax': 4, 'pragmatics': 3, 'etc': 2, 'two': 7, 'languages': 41, 'guess': 1, 'they': 10, 'main': 11, 'reason': 2, 'why': 3, 'both': 8, 'are': 25, 'appealing': 1, 'practical': 5, '-': 24, 'oriented': 1, 'prefer': 1, 'plain': 1, 'theory': 3, 'am': 23, 'master': 1, 'student': 5, 'english': 4, 'my': 46, 'motivation': 3, 'study': 20, 'scientific': 2, 'perspective': 2, 'as': 25, 'started': 10, 'program': 10, 'tuebingen': 2, 'university': 4, 'quantitative': 1, 'approaches': 1, 'therefore': 5, 'taking': 2, 'statistical': 4, 'courses': 1, 'what': 13, 'particularly': 2, 'formal': 4, 'semantics': 4, 'processing': 5, 'general': 7, 'our': 17, 'means': 2, 'expressing': 1, 'ourselves': 1, 'helps': 1, 'communicate': 3, 'express': 1, 'cultural': 1, 'identity': 1, 'sense': 5, 'nationality': 1, 'but': 24, 'arty': 1, 'way': 11, 'have': 23, 'been': 8, 'fascinated': 2, 'by': 23, '/': 6, 'dialect': 2, 'change': 5, 'rapidly': 1, 'even': 8, 'village': 2, 'next': 2, '\"': 11, 'teach': 3, 'world': 7, 'hardly': 1, 'imaginable': 1, 'without': 2, 'computers': 11, 'anymore': 2, 'task': 1, 'some': 9, 'learning': 28, 'like': 21, 'new': 13, 'learn': 22, 'about': 16, 'combine': 3, 'cause': 1, 'soon': 1, 'discovered': 2, 'iscl': 8, 'than': 8, 'combination': 2, 'its': 5, 'whole': 2, 'loved': 1, 'structuring': 1, 'organizing': 1, 'things': 9, 'lists': 1, 'notebooks': 1, 'build': 3, 'up': 7, 'own': 4, 'table': 1, 'contents': 1, 'perfectly': 1, 'fits': 1, 'this': 24, 'passion': 2, 'year': 1, '2000': 1, 'basic': 2, 'c': 2, 'founding': 1, 'startups': 1, '2004': 1, 'where': 12, 'we': 18, 'developed': 4, 'social': 2, 'network': 1, '>': 1, '1m': 1, 'users': 3, 'first': 8, 'encounter': 1, 'nlp': 4, 'when': 6, 'studied': 1, 'search': 3, 'queries': 2, 'entered': 1, 'other': 7, 'groups': 1, 'blogs': 1, 'on': 16, 'had': 11, 'format': 1, 'user': 1, 'city': 1, 'age': 3, 'asl': 1, '!': 2, 'rule': 2, 'based': 4, 'system': 4, 'ngram': 2, 'index': 1, 'sufficient': 1, 'after': 2, 'web': 1, 'analytics': 1, 'platform': 1, 'years': 2, 'personal': 1, 'interest': 23, 'lying': 1, 'sophisticated': 1, 'google': 2, 'referer': 1, 'clustering': 2, 'at': 9, 'naive': 1, 'bayes': 1, 'implemented': 1, 'sql': 1, 'then': 5, 'realized': 1, 'need': 5, 'background': 4, 'math': 4, 'went': 2, 'toy': 1, 'startup': 1, 'semester': 3, 'break': 1, 'apart': 1, 'email': 2, 'addresses': 1, 'semantic': 1, 'evaluation': 1, '(': 19, 'http': 1, '://': 1, 'api': 1, 'mailingbird': 1, 'com': 1, '/?': 1, '=': 1, 'robert': 1, 'eisele': 1, '@': 1, 'uni': 1, 'tuebinge': 1, 'de': 1, ')': 14, 'engine': 1, 'improves': 1, 'itself': 2, 'seeing': 1, 'firstname': 3, 'lastname': 1, 'combinations': 2, 'news': 3, 'articles': 2, 'wikipedia': 2, 'since': 11, 'name': 2, 'namefirstname': 1, 'firstamename': 1, 'possible': 5, 'brought': 2, 'sentiment': 1, 'analysis': 2, 'politics': 1, 'public': 2, 'media': 3, 'times': 1, 'dominates': 1, 'worth': 1, 'spreading': 1, 'hard': 1, 'say': 3, 'fake': 1, 'influences': 1, 'opinion': 4, 'major': 3, 'science': 10, 'theoretical': 4, 'formalism': 1, 'behind': 2, 'buzzwords': 1, 'approach': 1, 'type': 1, '3': 1, 'examples': 2, 'automatically': 1, 'translating': 2, 'csv': 1, 'html': 1, 'private': 1, 'research': 6, 'project': 1, 'finding': 1, 'temporal': 1, 'hierarchy': 1, 'dictionaries': 1, \"'\": 35, 'm': 15, 'still': 2, 'hunt': 1, 'dictionary': 1, 'sources': 2, 'human': 16, 'full': 1, 'knowledge': 5, 'fun': 4, 'discover': 2, 'these': 12, 'treasures': 1, 'looking': 3, 'forward': 2, 'course': 7, 'techniques': 1, 'such': 7, 'favourite': 1, 'area': 4, 'crucial': 1, 'yet': 2, 'often': 2, 'ignored': 1, 'aspect': 5, 'voice': 1, 'implementing': 2, 'chatbots': 1, 'done': 4, 'order': 2, 'improve': 4, 'them': 6, 'love': 3, 'do': 7, 'cognitive': 2, 'processes': 3, 'underlying': 4, 'surely': 2, 'quality': 3, 'interaction': 4, 'ba': 3, 'phonetics': 1, 'phonology': 3, 'well': 4, 'cognition': 1, 'underestanding': 1, 'using': 4, 'approchaces': 1, 'deep': 2, 'neural': 2, 'networks': 2, 'furthormore': 1, 'use': 13, 'ai': 6, 'algorithms': 2, 'interact': 2, 'machines': 4, 'smart': 1, 'phones': 1, 'game': 1, 'box': 1, 'xbox': 1, '),': 4, 'cars': 1, 'chose': 4, 'includes': 1, 'technical': 2, 'humanitarian': 2, 'subjects': 2, 'belongs': 1, 'computation': 20, 'studies': 3, 'only': 6, 'amazing': 1, 'acquire': 1, 'structure': 2, 'levels': 1, 'decided': 1, 'gave': 2, 'insight': 1, 'into': 7, 'occur': 1, 'within': 1, 'most': 11, 'possibility': 1, 'building': 1, 'models': 3, 'process': 6, 'generate': 1, 'seems': 5, 'great': 3, 'significance': 1, 'development': 8, 'artificial': 2, 'intelligence': 3, 'consider': 2, 'important': 2, 'goal': 2, 'today': 1, 'came': 2, 'somewhat': 2, 'serendipitously': 1, 'short': 2, 'observed': 1, 'workplace': 1, 'programmer': 2, 'skills': 2, 'proved': 1, 'best': 1, 'fit': 1, 'taught': 1, 'living': 3, 'germany': 4, 'come': 6, 'greatly': 1, 'appreciate': 1, 'translate': 2, 'now': 5, 'amazon': 1, 'alexa': 1, 'regularly': 1, 'excited': 2, 'potential': 1, 'interfaces': 3, 'provide': 1, 'healthier': 1, 'humans': 3, 'everyday': 2, 'opposed': 1, 'addictive': 1, 'distracting': 1, 'screens': 1, 'become': 3, 'ways': 4, 'technologies': 1, 'could': 8, 'influence': 1, 'life': 3, 'through': 2, 'interface': 1, 'telling': 1, 'driver': 1, 'tyre': 1, 'pressure': 1, 'streamlining': 1, 'agricultural': 1, 'practices': 1, 'allowing': 1, 'farmers': 1, 'record': 1, 'vocal': 1, 'data': 5, 'while': 2, 'their': 7, 'hands': 1, 'dirty': 1, 'wording': 1, 'people': 3, 'certain': 2, 'context': 1, 'sentences': 3, 'meaning': 4, 'misunderstood': 1, 'being': 7, 'connected': 2, 'fact': 2, 'themselves': 1, 'grammar': 1, 'carry': 2, 'used': 1, 'interpret': 1, 'sentence': 2, 'leads': 1, 'back': 6, 'question': 3, 'interdisciplinarity': 1, 'fascinates': 1, 'easy': 1, 'tools': 4, 'makes': 3, 'text': 3, 'alter': 1, 'fast': 2, 'make': 11, 'communication': 4, 'easier': 3, 'sometimes': 1, 'all': 7, 'over': 4, 'ü': 1, 'idea': 3, 'relationship': 2, 'between': 10, 'regular': 1, 'achieved': 1, 'creating': 1, 'software': 4, 'products': 2, 'going': 3, 'lives': 3, 'urgently': 1, 'needed': 1, 'obstacle': 1, 'problem': 2, 'programs': 3, 'will': 6, 'broad': 2, 'set': 2, 'applications': 2, 'spoken': 3, 'information': 1, 'systems': 7, 'others': 1, 'enables': 1, 'better': 5, 'understanding': 3, 'works': 1, 'able': 6, 'connection': 1, 'part': 4, 'focuses': 1, 'relation': 2, 'form': 2, 'sake': 1, 'has': 4, 'properties': 1, 'relates': 2, 'aspects': 5, 'computing': 1, '\\ufeff': 5, 'essential': 1, 'respectivly': 1, 'teaching': 1, 'artifical': 1, 'especially': 4, 'ones': 2, 'capable': 1, 'destroying': 1, 'blockade': 1, 'any': 5, 'input': 1, 'let': 2, 'existing': 2, 'further': 2, 'imagine': 1, 'example': 6, 'an': 8, 'interactive': 1, 'entire': 1, 'knowlegde': 1, 'every': 3, 'published': 1, 'paper': 1, 'there': 8, 'tell': 1, 'read': 1, 'start': 1, 'time': 3, 'spend': 1, 'working': 2, 'control': 2, 'department': 2, 'big': 1, 'company': 1, 'homepage': 1, 'testing': 1, 'automation': 1, 'spell': 1, 'checking': 1, 'doing': 2, 'visual': 2, 'checks': 2, 'corporate': 1, 'pages': 1, 'okay': 1, 'translations': 1, 'were': 3, 'given': 2, 'lower': 1, 'freedom': 1, 'texts': 1, 'explain': 1, 'homepages': 1, 'created': 1, 'companies': 1, 'cms': 1, 'support': 1, 'having': 3, 'huge': 2, 'vital': 1, 'allows': 2, 'enjoy': 2, 'facets': 1, 'bridge': 2, 'gap': 2, 'disciplines': 1, 'finally': 2, 'give': 2, 'instruction': 1, 'application': 1, 'sure': 1, 'rather': 3, 'yeah': 1, 'early': 1, 'analyzing': 2, 'simplifying': 1, 'see': 2, 'structures': 2, 'java': 2, 'python': 2, 'real': 2, 'vice': 1, 'versa': 1, 'developing': 1, 'large': 3, 'uses': 2, 'future': 6, 'goes': 2, '2011': 1, 'presented': 1, 'bachelor': 3, 'thesis': 1, 'localization': 2, 'food': 1, 'labels': 1, 'fulfill': 1, 'en': 1, 'pt': 1, 'brasilia': 1, 'brazil': 2, 'purposed': 1, 'acquainted': 1, 'technological': 1, 'involved': 2, 'websites': 1, 'apps': 1, 'softwares': 1, 'sparkled': 1, 'exploring': 1, 'technology': 4, 'result': 1, 'ma': 1, 'abovementioned': 1, 'explore': 1, 'measuring': 1, 'activity': 1, 'platforms': 1, 'twitter': 1, 'whereby': 1, 'ultimately': 1, 'across': 1, 'impact': 1, 'caused': 1, 'growing': 1, 'tech': 2, 'gadgets': 1, 'aligned': 1, 'boundaries': 1, 'parts': 3, 'develop': 2, 'areas': 2, 'almost': 2, 'speak': 3, 'e': 2, 'g': 1, 'ireland': 1, 'papua': 1, 'guinea': 1, ')?': 1, 'particular': 3, 'difference': 1, 'native': 2, 'foreign': 1, 'think': 5, 'importance': 1, 'environment': 2, 'children': 1, 'lead': 2, 'implications': 1, 'structur': 1, 'brain': 2, 'modelling': 1, 'developments': 2, 'sounds': 1, 'biggest': 2, 'took': 1, 'not': 11, 'class': 2, 'specifically': 2, 'live': 1, 'second': 5, 'consideration': 1, 'marketability': 1, ';': 3, 'already': 2, 'german': 4, 'degree': 5, 'biology': 1, 'america': 1, 'before': 5, 'ran': 1, 'out': 2, 'money': 1, 'point': 3, 'concerned': 2, 'primarily': 1, 'ability': 2, 'paying': 1, 'job': 4, 'obtain': 1, 'visa': 1, 'extremely': 1, 'useful': 2, 'making': 2, 'competitive': 1, 'market': 1, 'figured': 2, 'seemed': 4, 'step': 1, 'friends': 2, 'home': 1, 'usa': 1, 'jobs': 1, 'mid': 1, '20s': 1, 'earn': 1, 'parents': 2, 'logical': 3, 'same': 1, 'did': 3, 'put': 2, 'layman': 1, 'professionally': 1, 'combines': 2, 'much': 5, 'solving': 3, 'tricky': 1, 'coming': 1, 'solutions': 1, 'problems': 2, 'moreover': 2, 'enjoyed': 1, 'analysing': 1, 'intersection': 1, 'provides': 2, 'hope': 3, 'believe': 2, 'assistance': 1, 'analyze': 2, 'amount': 1, 'alone': 1, 'couldn': 2, 'refines': 1, 'regardless': 1, 'feel': 2, 'linguists': 1, 'kind': 2, 'made': 1, 'theories': 2, 'fix': 1, 'assumption': 1, 'lanagues': 1, 'skeptical': 1, 'rules': 3, 'know': 3, 'interests': 3, 'present': 1, 'comprehension': 1, 'production': 1, 'cl': 1, 'education': 1, 'psycholinguistics': 1, 'us': 4, 'kinda': 1, 'want': 4, 'if': 4, 'enough': 2, 'yes': 1, 'aster': 1, 'verw': 1, 'engineering': 1, 'delve': 1, 'links': 1, 'ebtween': 1, 'realms': 1, 'combining': 2, 'parsers': 1, 'mistakes': 1, 'overall': 1, 'helpful': 1, 'tool': 1, 'day': 3, 'basis': 1, 'points': 1, 'those': 2, 'topics': 1, 'lot': 4, 'concepts': 1, 'projects': 1, 'currently': 1, 'intelligent': 2, 'tutoring': 1, 'benefit': 1, 'matches': 1, 'weaknesses': 1, 'financially': 1, 'advantaged': 1, 'disadvantaged': 1, 'students': 1, 'topic': 3, 'intriguing': 2, 'statistics': 2, 'recent': 1, 'suggests': 1, 'probability': 1, 'pretty': 3, 'state': 1, 'art': 1, 'plays': 1, 'un': 1, 'doubtedly': 1, 'happening': 1, 'young': 2, 've': 4, 'motivated': 1, 'learner': 1, 'enjoying': 1, 'sorts': 1, 'anagrams': 1, 'playing': 1, 'scrabble': 1, 'writing': 3, 'rhymes': 1, 'list': 1, 'mind': 4, 'sticking': 1, 'appropriate': 1, 'choice': 2, 'technically': 1, 'artistically': 1, 'inclined': 1, 'opted': 1, 'literature': 1, 'experience': 2, 'high': 1, 'thought': 5, 'sensible': 1, 'terms': 1, 'career': 2, 'keep': 1, 'perfect': 2, 'friend': 2, 'connecting': 1, 'beautiful': 1, 'little': 1, 'three': 1, 'semesters': 1, 'increasingly': 1, 'leaning': 1, 'towards': 1, 'side': 4, 'strong': 1, 'liking': 1, 'mathematics': 2, 'previously': 1, 'considered': 1, 'too': 3, 'mechanical': 1, 'hugely': 2, 'attempt': 1, 'patterns': 1, 'combined': 2, 'complex': 3, 'definitely': 1, 'pass': 1, 'whether': 1, 'reliably': 1, 'deconstruct': 1, 'atomic': 1, 'components': 1, 'whatever': 1, 'may': 3, 'philosophical': 1, 'answer': 1, 'probably': 1, 'no': 6, 'never': 3, 'decode': 1, 'fully': 1, 'enticing': 1, 'convoluted': 1, 'ever': 1, 'solution': 1, 'smallest': 1, 'unexplored': 1, 'sub': 1, 'equation': 1, 'seismic': 1, 'event': 1, 'anyone': 1, 'focus': 1, 'mostly': 2, 'challenges': 1, 'apply': 2, 'methods': 1, 'snlp': 1, 'should': 1, 'perspectives': 1, 'here': 1, 'initially': 1, 'desire': 1, 'comprehend': 1, 'child': 1, 'engaging': 1, 'crossed': 1, 'until': 2, 'current': 1, 'necessities': 1, 'everything': 1, 'ongoing': 1, 'curious': 1, 'happens': 1, 'curtains': 1, 'constantly': 1, 'improved': 2, 'services': 1, 'replaced': 1, 'frightened': 1, 'considerably': 1, 'pursue': 1, 'family': 2, 'travelling': 1, 'financial': 1, 'reward': 1, 'longer': 1, 'primary': 1, 'goals': 1, 'robots': 1, 'place': 2, 'servants': 1, 'simply': 1, 'execute': 1, 'consuming': 1, 'repetitive': 1, 'create': 1, 'resource': 1, 'revenue': 1, 'sustain': 1, 'choices': 1, 'preferences': 1, 'factor': 1, 'survive': 1, '!“': 1, 'exist': 1, 'reasons': 1, 'foundation': 1, 'cool': 1, 'found': 3, 'fascinating': 2, 'facilitate': 1, 'brainer': 1, 'acquisition': 1, 'acquiring': 1, 'third': 2, 'lexical': 1, 'level': 1, 'linguisics': 1, 'tracking': 1, 'origin': 1, 'fining': 1, 'meanings': 1, 'relate': 1, 'extract': 1, 'informations': 1, 'thinking': 1, 'passions': 1, 'mine': 4, 'differently': 1, 'explained': 1, 'understand': 2, 'convinced': 1, 'right': 1, 'subject': 1, 'promising': 1, 'long': 1, 'remember': 1, 'bought': 1, 'unique': 1, 'kid': 2, 'grew': 1, 'internet': 1, 'classmates': 1, 'later': 1, 'basically': 1, 'lived': 1, 'front': 1, '#': 1, 'deciding': 1, 'tübingen': 1, 'deeper': 1, 'grown': 1, 'couple': 1, 'moved': 1, 'around': 1, 'few': 2, 'countries': 1, 'local': 1, 'actually': 1, 'casual': 1, 'becoming': 1, 'sort': 1, 'hobby': 1, 'lies': 1, 'historical': 1, 'investigating': 1, 'amazigh': 1, 'berber': 2, 'arabic': 2, 'although': 1, 'nor': 2, 'familiar': 1, 'aid': 1, 'd': 1, 'investigate': 1, 'ease': 1, 'journey': 2, 'taken': 1, 'foot': 1, 'whatsoever': 1, 'physical': 1, 'locomotion': 1, 'your': 1, 'preference': 1, 'wander': 1, 'thoroughfares': 1, 'spectacle': 1, 'unbridled': 1, 'cerebration': 1, 'odyssey': 1, 'land': 1, 'though': 1, 'majority': 1, 'cases': 1, 'sea': 1, 'conveyed': 1, 'far': 1, 'greater': 2, 'haste': 2, 'aided': 1, 'stalwart': 1, 'steed': 1, 'does': 1, 'practice': 1, 'unparalleled': 1, 'upon': 1, 'endeavors': 1, 'who': 2, 'among': 1, 'claim': 1, 'achievement': 1, 'take': 1, 'hesitant': 1, 'steps': 1, 'along': 1, 'path': 1, 'perhaps': 1, 'soul': 2, '?': 2, 'captcha': 1, 'test': 1, 'god': 1, 'fool': 1, 'wretched': 1, 'above': 1, 'hallowed': 1, 'favorite': 1, 'compilers': 1, 'designing': 1, 'obvious': 1, 'shapes': 1, 'care': 1, 'formalize': 1, 'compute': 1, 'during': 1, 'define': 1, 'changed': 2, 'morphemes': 1, 'seek': 1, 'changes': 1, 'hand': 1, 'eyes': 1, 'difficult': 2, 'differences': 1, 'elegant': 1, 'addition': 1, '\\u200b\\u200b': 1, 'compared': 1, 'write': 3, 'everybody': 1, 'decompose': 1, 'parse': 2, 'analyse': 1, 'corpus': 1, 'save': 1, 'necessary': 1, 'debug': 1, 'challenging': 1, 'efficient': 1, 'resources': 1, 'prior': 1, '2016': 1, 'commenced': 1, 'either': 1, 'begining': 1, 'winter': 1, 'asked': 1, 'her': 1, 'said': 2, 'however': 2, 'persuasion': 1, 'boring': 1, 'try': 1, 'expectations': 1, 'clue': 1, 'meant': 1, 'finsihed': 1, '1st': 1, 'definately': 1, 'frustrating': 1, 'satisfying': 1, 'enjoyable': 1, 'introduced': 1, 'complexity': 1, 'intrigued': 1, 'broadly': 1, 'functions': 1, 'cs': 1, 'regards': 1, 'trying': 1, 'codify': 1, 'last': 1, 'summer': 1, 'lab': 1, 'audio': 1, 'inspired': 1, 'largely': 1, 'surrounds': 1, '?\",': 1}\n"
    }
   ],
   "source": [
    "print(counts_1grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_2grams = {}\n",
    "for sentence in corpus:\n",
    "    ngram_update(counts_2grams, ngram_sentence(2, sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_3grams = {}\n",
    "for sentence in corpus:\n",
    "    ngram_update(counts_3grams, ngram_sentence(3, sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, compute n-gram probabilities, i.e. conditional probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the probability of each token in the corpus\n",
    "prob_1grams = {}\n",
    "prob_1grams = {ngram : (counts_1grams[ngram] / len(all_tokens)) for ngram in counts_1grams.keys()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0.08183530217428522"
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_1grams[\"<s>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0.0280773840095874"
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_1grams[\"i\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0.0001712035610340695"
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_1grams[\"you\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the probability of the second token, given the first, in each 2-gram\n",
    "prob_2grams = {}\n",
    "prob_2grams = {ngram : (counts_2grams[ngram] / counts_1grams[ngram[0]]) for ngram in counts_2grams.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0.1192468619246862"
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_2grams[(\"<s>\", \"i\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0.1402439024390244"
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_2grams[(\"i\", \"am\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0.012195121951219513"
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_2grams[(\"i\", \"want\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the probability of the third token, given the first two, in each 3-gram\n",
    "prob_3grams = {}\n",
    "prob_3grams = {ngram : (counts_3grams[ngram] / counts_2grams[(ngram[0], ngram[1])]) for ngram in counts_3grams.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0.2384937238493724"
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_3grams[(\"<s>\", \"<s>\", \"i\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0.2807017543859649"
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_3grams[(\"<s>\", \"i\", \"am\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0.16666666666666666"
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_3grams[(\"and\", \"i\", \"am\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we write a function which computes the MLE-probability of a tokenized sentence in a n-gram model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_prob(n, sentence):\n",
    "    p_sentence = 1\n",
    "    ngrams = ngram_sentence(n, sentence)\n",
    "\n",
    "    if n == 1:\n",
    "        for ngram in ngrams:\n",
    "            if ngram in prob_1grams.keys():\n",
    "                p_ngram = prob_1grams[ngram]\n",
    "            else:\n",
    "                p_ngram = 0\n",
    "            p_sentence = p_sentence * p_ngram\n",
    "\n",
    "    elif n == 2:\n",
    "        for ngram in ngrams:\n",
    "            if ngram in prob_2grams.keys():\n",
    "                p_ngram = prob_2grams[ngram]\n",
    "            else:\n",
    "                p_ngram = 0\n",
    "            p_sentence = p_sentence * p_ngram\n",
    "\n",
    "    else:\n",
    "        for ngram in ngrams:\n",
    "            if ngram in prob_3grams.keys():\n",
    "                p_ngram = prob_3grams[ngram]\n",
    "            else:\n",
    "                p_ngram = 0\n",
    "            p_sentence = p_sentence * p_ngram\n",
    "    \n",
    "    return(p_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "1.7192490764201554e-92"
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = corpus[0]\n",
    "sentence_prob(1, sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "4.930060929596075e-38"
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_prob(2, sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "1.4412823861917303e-09"
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_prob(3, sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a similar function to computed smoothed probability, so that we can evaluate sentences containing unseen n-grams as well.\n",
    "\n",
    "First, we need to compute the smoothed probabilities of ngrams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the laplace-smoothed (=add one) probability of each token in the corpus\n",
    "sprob_1grams = {}\n",
    "sprob_1grams = {ngram : ((counts_1grams[ngram]+1) / (len(all_tokens)+len(vocab))) for ngram in counts_1grams.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in order to compute the good-turing-smoothed probability of n-grams, we need a function that, for each rate r\n",
    "# (e.g. r=1 means once, r=2 twice etc) tells us how many n-grams occured with that rate r in the corpus\n",
    "\n",
    "def rate_ngram(r, ngrams): # r is the rate, ngrams is a dictionary with counts of n-grams\n",
    "    c = 0\n",
    "    for key,value in ngrams.items():\n",
    "        if value == r:\n",
    "            c = c + 1\n",
    "    return(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "315"
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rate_ngram(2, counts_2grams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, for each ngram in the dictionary, we compute its probability as `(r+1)*(N_(r+1))/(N*N_r)` where `r` is the rate of the n-gram, `N_(r)` is the result of `rate_ngram(r, dictionary)` defined above, and similarly `N_(r+1) = rate_ngram(r+1, dictionary)`; `N_ngrams` is the total number of n-grams: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "sprob_2grams = {}\n",
    "N_2grams = len(counts_2grams.keys())\n",
    "for ngram in counts_2grams.keys():\n",
    "    r = counts_2grams[ngram]\n",
    "    N_r = rate_ngram(r, counts_2grams)\n",
    "    N_r1 = rate_ngram(r+1, counts_2grams)\n",
    "\n",
    "    sprob_2grams[ngram] = (r+1)*(N_r1)/(N_2grams*N_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same for corpus of 3-grams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "sprob_3grams = {}\n",
    "N_3grams = len(counts_3grams.keys())\n",
    "for ngram in counts_3grams.keys():\n",
    "    r = counts_3grams[ngram]\n",
    "    N_r = rate_ngram(r, counts_3grams)\n",
    "    N_r1 = rate_ngram(r+1, counts_3grams)\n",
    "\n",
    "    sprob_3grams[ngram] = (r+1)*(N_r1)/(N_3grams*N_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the smoothed probability function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_sprob(n, sentence):\n",
    "    p_sentence = 1\n",
    "    ngrams = ngram_sentence(n, sentence)\n",
    "\n",
    "    if n == 1:\n",
    "        for ngram in ngrams:\n",
    "            if ngram in sprob_1grams.keys():\n",
    "                p_ngram = sprob_1grams[ngram]\n",
    "            else:\n",
    "                p_ngram = 1/len(all_tokens)\n",
    "            p_sentence = p_sentence * p_ngram\n",
    "\n",
    "    elif n == 2:\n",
    "        for ngram in ngrams:\n",
    "            if ngram in sprob_2grams.keys():\n",
    "                p_ngram = sprob_2grams[ngram]\n",
    "            else:\n",
    "                p_ngram = rate_ngram(1, counts_2grams)/(N_2grams)\n",
    "            p_sentence = p_sentence * p_ngram\n",
    "\n",
    "    else:\n",
    "        for ngram in ngrams:\n",
    "            if ngram in sprob_3grams.keys():\n",
    "                p_ngram = sprob_3grams[ngram]\n",
    "            else:\n",
    "                p_ngram = rate_ngram(1, counts_3grams)/(N_3grams)\n",
    "            p_sentence = p_sentence * p_ngram\n",
    "    \n",
    "    return(p_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "8.438168826593244e-10"
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_sprob(1, [\"i\", \"love\", \"computational\", \"linguistics\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "2.5400462830607033e-10"
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_sprob(1, [\"i\", \"luv\", \"computational\", \"linguistics\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "2.4818143607627192e-11"
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_sprob(1, [\"i\", \"love\", \"computational\", \"biology\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "3.9110042531093435e-13"
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_sprob(1, [\"i\", \"love\", \"computerazible\", \"languageness\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With 2-grams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0.005228428326151054"
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_sprob(2, [\"i\", \"love\", \"computational\", \"linguistics\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0.005228428326151054"
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_sprob(2, [\"i\", \"luv\", \"computational\", \"linguistics\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0.6083649816642906"
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_sprob(2, [\"i\", \"love\", \"computational\", \"biology\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0.6083649816642906"
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_sprob(2, [\"i\", \"love\", \"computerazible\", \"languageness\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With 3-grams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0.8983096627467306"
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_sprob(3, [\"i\", \"love\", \"computational\", \"linguistics\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0.8983096627467306"
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_sprob(3, [\"i\", \"luv\", \"computational\", \"linguistics\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0.8983096627467306"
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_sprob(3, [\"i\", \"love\", \"computational\", \"biology\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0.8983096627467306"
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_sprob(3, [\"i\", \"love\", \"computerazible\", \"languageness\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can this be right?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, perplexity. We compute the perplexity of testing corpus, using the smoothed probability above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, read test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path_test = \"data/test/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_test, all_tokens_test, vocab_test = tokenize(input_path_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, for each model, compute smoothed probability of each sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sprob_1grams = [sentence_sprob(1, sentence) for sentence in corpus_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sprob_2grams = [sentence_sprob(2, sentence) for sentence in corpus_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sprob_3grams = [sentence_sprob(3, sentence) for sentence in corpus_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probability of full corpus is the product of the probabilities of the sentences; then perplexity is computed as `P_corpus^(-1/N)` where `N` is the number of tokens in the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "inf\n"
    }
   ],
   "source": [
    "P_corpus_1gram = np.prod(test_sprob_1grams)\n",
    "test_ppl_1gram = P_corpus_1gram**(-(1/len(all_tokens_test)))\n",
    "print(test_ppl_1gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "inf\n"
    }
   ],
   "source": [
    "P_corpus_2gram = np.prod(test_sprob_2grams)\n",
    "test_ppl_2gram = P_corpus_2gram**(-(1/len(all_tokens_test)))\n",
    "print(test_ppl_2gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "inf\n"
    }
   ],
   "source": [
    "P_corpus_3gram = np.prod(test_sprob_3grams)\n",
    "test_ppl_3gram = P_corpus_3gram**(-(1/len(all_tokens_test)))\n",
    "print(test_ppl_3gram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This doesn't look good :D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "Neural language model. Let's try with this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}