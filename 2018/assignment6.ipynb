{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37664bitbasevenv23cd022687ab470688da671eade16340",
   "display_name": "Python 3.7.6 64-bit ('base': venv)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 6\n",
    "## Language model\n",
    "The assignment and data are available here: https://snlp2018.github.io/assignments.html\n",
    "\n",
    "The `train` folder in the archive contains several text file produced by students of the course as self-introduction; the `test` folder contains a few text files of the same kind."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excercise 1\n",
    "Tokenization. Using `nltk`, we write a function which takes a variable number of text files as input and returns a tokenize corpus (list of lists of tokens) and the corpus' vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import wordpunct_tokenize\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"data/train/\" # this is the argument of the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(path_to_files):\n",
    "    input_files = glob(input_path + \"*.txt\") # get all the txt files in the folder\n",
    "\n",
    "    texts = [] # initialize empty list where we collect data\n",
    "    for input_file in input_files:\n",
    "        with open(input_file, \"r\", encoding = \"utf-8\") as in_file: # read the file\n",
    "            text = in_file.read().split(\".\") # split sentences\n",
    "            for line in text:\n",
    "                tknzd_line = wordpunct_tokenize(line.lower()) # lowercase and tokenize each sentence\n",
    "                if tknzd_line != []:\n",
    "                    texts.append(tknzd_line)\n",
    "\n",
    "    vocab = sorted(set([token for sentence in texts for token in sentence])) # list of unique tokens\n",
    "    \n",
    "    return(texts, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus, vocab = tokenize(input_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[['i', 'always', 'wanted', 'to', 'work', 'with', 'words', 'or', 'language', ',', 'and', 'it', 'didn', '’', 't', 'matter', 'for', 'me', 'would', 'it', 'be', 'something', 'related', 'to', 'neurolinguistics', ',', 'just', 'a', 'simple', 'translation', 'work', 'or', 'anything', 'more', 'computational']]\n"
    }
   ],
   "source": [
    "print(corpus[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "['!', '!“', '\"', '#', \"'\", '(', ')', '),', ')?', ',', '-', '/', '/?', '1m', '1st', '2000', '2004', '2011', '2016', '20s', '3', ':', '://', ';', '=', '>', '?', '?\",', '@', 'a', 'ability', 'able', 'about', 'above', 'abovementioned', 'access', 'achieved', 'achievement', 'acquainted', 'acquire', 'acquiring', 'acquisition', 'across', 'activity', 'actually', 'addictive', 'addition', 'addresses', 'advantaged', 'after', 'age', 'agricultural', 'ai', 'aid', 'aided', 'alexa', 'algorithms', 'aligned', 'all', 'allow', 'allowing', 'allows', 'almost', 'alone', 'along', 'already', 'also', 'alter', 'although', 'always', 'am', 'amazigh', 'amazing', 'amazon', 'america', 'among', 'amount', 'an', 'anagrams', 'analyse', 'analysing', 'analysis', 'analytics', 'analyze', 'analyzing', 'and', 'another', 'answer', 'any', 'anymore', 'anyone', 'anything', 'apart', 'api', 'appealing', 'application', 'applications', 'apply', 'appreciate', 'approach']\n"
    }
   ],
   "source": [
    "print(vocab[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shall we remove punctuation as pre-processing? We'll see..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excercise 2\n",
    "N-gram language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}