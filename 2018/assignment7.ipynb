{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37664bitbasevenv23cd022687ab470688da671eade16340",
   "display_name": "Python 3.7.6 64-bit ('base': venv)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 7\n",
    "## GermEval 2018 shared task: identification of offensive language\n",
    "\n",
    "GermEval task: https://projects.fzai.h-da.de/iggsa/germeval-2018/\n",
    "\n",
    "Assignment: https://snlp2018.github.io/assignments.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1\n",
    "The goal of Task 1 is to detect offensive language in social media posts in German: tweets have to be classified as either `OFFENSE` or `OTHER`.\n",
    "\n",
    "Data: https://github.com/uds-lsv/GermEval-2018-Data\n",
    "\n",
    "The file `germeval2018.training.txt` is a tab-separated list of labelled tweets; this is the development set to be used in training and tuning of model(s). The file `germeval2018.test.txt` contains the \"gold-standard\" data against which the final model should be (and has been, in the actual shared task) evaluated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The data\n",
    "First of all, let's read and take a quick look at the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/germeval2018.training.txt\", sep = \"\\t\", encoding = \"utf-8\", header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                                   0        1       2\n0  @corinnamilborn Liebe Corinna, wir w√ºrden dich...    OTHER   OTHER\n1  @Martin28a Sie haben ja auch Recht. Unser Twee...    OTHER   OTHER\n2  @ahrens_theo fr√∂hlicher gru√ü aus der sch√∂nsten...    OTHER   OTHER\n3  @dushanwegner Amis h√§tten alles und jeden gew√§...    OTHER   OTHER\n4  @spdde kein verl√§√ülicher Verhandlungspartner. ...  OFFENSE  INSULT",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>@corinnamilborn Liebe Corinna, wir w√ºrden dich...</td>\n      <td>OTHER</td>\n      <td>OTHER</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>@Martin28a Sie haben ja auch Recht. Unser Twee...</td>\n      <td>OTHER</td>\n      <td>OTHER</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>@ahrens_theo fr√∂hlicher gru√ü aus der sch√∂nsten...</td>\n      <td>OTHER</td>\n      <td>OTHER</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>@dushanwegner Amis h√§tten alles und jeden gew√§...</td>\n      <td>OTHER</td>\n      <td>OTHER</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>@spdde kein verl√§√ülicher Verhandlungspartner. ...</td>\n      <td>OFFENSE</td>\n      <td>INSULT</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe: first column contains tweet and second column the labels `OFFENSE` or `OTHER`; third column contains finer-grained labeling which we are not interesed in at the moment. Let's drop the latter and give the other two columns user-friendly names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(df.columns[2], axis = 1, inplace = True)\n",
    "df.columns = [\"tweet\", \"category\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                               tweet category\n0  @corinnamilborn Liebe Corinna, wir w√ºrden dich...    OTHER\n1  @Martin28a Sie haben ja auch Recht. Unser Twee...    OTHER\n2  @ahrens_theo fr√∂hlicher gru√ü aus der sch√∂nsten...    OTHER\n3  @dushanwegner Amis h√§tten alles und jeden gew√§...    OTHER\n4  @spdde kein verl√§√ülicher Verhandlungspartner. ...  OFFENSE",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweet</th>\n      <th>category</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>@corinnamilborn Liebe Corinna, wir w√ºrden dich...</td>\n      <td>OTHER</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>@Martin28a Sie haben ja auch Recht. Unser Twee...</td>\n      <td>OTHER</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>@ahrens_theo fr√∂hlicher gru√ü aus der sch√∂nsten...</td>\n      <td>OTHER</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>@dushanwegner Amis h√§tten alles und jeden gew√§...</td>\n      <td>OTHER</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>@spdde kein verl√§√ülicher Verhandlungspartner. ...</td>\n      <td>OFFENSE</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is the data balanced? Let's find out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "          tweet\ncategory       \nOFFENSE    1688\nOTHER      3321",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweet</th>\n    </tr>\n    <tr>\n      <th>category</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>OFFENSE</th>\n      <td>1688</td>\n    </tr>\n    <tr>\n      <th>OTHER</th>\n      <td>3321</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "df.groupby(\"category\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not quite, there are twice as many tweets labelled as `OTHER` than tweets labelled as `OFFENSE`. Is this a problem? It can be; so: 1) we won't use accuracy but precision and recall metrics and 2) we might need to weight the loss function(s) of our model(s). We'll come back to this later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing\n",
    "\n",
    "What do the tweets look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['@NeudF Hast Du nichts anderes drauf?', 'User die melden sind einfach Schmutz', '@IQ_Stimulator @Boelscheline Andere m√∂gen es ignorieren.....wir nicht ..!!', \"Gleich geht's los! Wir dr√ºcken die Daumen f√ºr @imriziv, der in K√ºrze als 18. f√ºr #Israel an den Start geht. #Eurovision #ESC2017 üáÆüá±üáÆüá±üáÆüá±\", '@ShakRiet @correctiv_org Das wurde in der DDR und BRD getan &gt; BIS HEUTE!', '@Christo75554365 Doch das hat mit dem religi√∂sen Weltbild zu tun, nachdem der K√∂rper einer Frau per se etwas S√ºndiges ist. Aber f√ºr jeden vern√ºnftig denkenden Zeitgenossen ist klar: der K√∂rper ist neutral. Nur Gedanken k√∂nnen schmutzig sein.', '#Mallorca, einheimische Arbeitslose bekommen nichts. Wirtschaftsmigranten bekommen Kindergeld und Sozialhilfe.', '@EstoyLimpia @KumaAndrea @Moni4950 @loriotfehlt Ja. Aber eine Islamisierung findet nicht statt. Wenn wir alle brav Blockfl√∂te spielen üòò', 'WIE W√ÑRE ES MAL MIT GENERALVERDACHT GEGEN√úBER MUSLIMISCHISLAMISTISCHER EROBERER ??? Jeden Tag mehrere Kapitalverbrechen von JENEN!', '@AfDFraktionAGH @DuHugonotte Hier sieht man wie die Polizei in DE angesehen ist']\n"
    }
   ],
   "source": [
    "import random\n",
    "print(random.sample(df[\"tweet\"].to_list(), 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we'd expect, there are Twitter handles to users (`@...`), hashtags (`#...`) and emojis, beside more or less usual punctuation. For a starter, we can perhaps remove the handles (Twitter's usernames are typically not German words) but keep hashtags and emojis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"@handle1 @handle2 some other words or #hashtags @handle3 and finally the end @handle4.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "some other words or #hashtags  and finally the end .\n"
    }
   ],
   "source": [
    "tmp = re.sub(r\"\\@\\w+\",\"\", string)\n",
    "print(tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can remove leftover multiple blank spaces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "some other words or #hashtags and finally the end .\n"
    }
   ],
   "source": [
    "tmp1 = re.sub(r\"\\s+\", \" \", tmp)\n",
    "print(tmp1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's a write a simple function to do this cleaning and apply it to the `tweet` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_cleaner(string):\n",
    "    clean_string = re.sub(r\"\\s+\", \" \", re.sub(r\"\\@\\w+\",\"\", string))\n",
    "    return(clean_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Before: @handle1 @handle2 some other words or #hashtags @handle3 and finally the end @handle4.\nAfter:  some other words or #hashtags and finally the end .\n"
    }
   ],
   "source": [
    "print(\"Before: \" + string)\n",
    "print(\"After: \" + tweet_cleaner(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:, \"clean_tweet\"] = df.apply(lambda row : tweet_cleaner(row.tweet), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                               tweet category  \\\n0  @corinnamilborn Liebe Corinna, wir w√ºrden dich...    OTHER   \n1  @Martin28a Sie haben ja auch Recht. Unser Twee...    OTHER   \n2  @ahrens_theo fr√∂hlicher gru√ü aus der sch√∂nsten...    OTHER   \n3  @dushanwegner Amis h√§tten alles und jeden gew√§...    OTHER   \n4  @spdde kein verl√§√ülicher Verhandlungspartner. ...  OFFENSE   \n\n                                         clean_tweet  \n0   Liebe Corinna, wir w√ºrden dich gerne als Mode...  \n1   Sie haben ja auch Recht. Unser Tweet war etwa...  \n2   fr√∂hlicher gru√ü aus der sch√∂nsten stadt der w...  \n3   Amis h√§tten alles und jeden gew√§hlt...nur Hil...  \n4   kein verl√§√ülicher Verhandlungspartner. Nachka...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweet</th>\n      <th>category</th>\n      <th>clean_tweet</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>@corinnamilborn Liebe Corinna, wir w√ºrden dich...</td>\n      <td>OTHER</td>\n      <td>Liebe Corinna, wir w√ºrden dich gerne als Mode...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>@Martin28a Sie haben ja auch Recht. Unser Twee...</td>\n      <td>OTHER</td>\n      <td>Sie haben ja auch Recht. Unser Tweet war etwa...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>@ahrens_theo fr√∂hlicher gru√ü aus der sch√∂nsten...</td>\n      <td>OTHER</td>\n      <td>fr√∂hlicher gru√ü aus der sch√∂nsten stadt der w...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>@dushanwegner Amis h√§tten alles und jeden gew√§...</td>\n      <td>OTHER</td>\n      <td>Amis h√§tten alles und jeden gew√§hlt...nur Hil...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>@spdde kein verl√§√ülicher Verhandlungspartner. ...</td>\n      <td>OFFENSE</td>\n      <td>kein verl√§√ülicher Verhandlungspartner. Nachka...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline models\n",
    "\n",
    "Let's start quick and easy: let's see how vanilla Logistic Regression, Naive Bayes, Support Vector Machines behave with `tf-idf` features, using `sklearn`'s pipelines.\n",
    "\n",
    "First, we split the `df` into training and testing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.clean_tweet # features are the cleaned tweets\n",
    "y = df.category # labels are the categories\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 42) # split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above, let's also compute sample weights in order to \"cure\" unbalanced target labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = class_weight.compute_sample_weight(class_weight = \"balanced\", y = df.category) # weights for the full dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, define a pipeline with vectorizer, tf-idf encoder and logistic regression model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = Pipeline([('vect', CountVectorizer()), # transform text into matrix of token counts\n",
    "                   ('tfidf', TfidfTransformer()), # from token counts to normalized tf-idf\n",
    "                   ('clf', LogisticRegression()), # logistic regression classifier\n",
    "                  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg.fit(X_train, y_train, clf__sample_weight = weights[0 : len(y_train)]) # training, with weighted samples\n",
    "y_pred = logreg.predict(X_test) # predict labels of test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can quickly explore model's performance with the help of `sklearn`'s built-in metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "precision    recall  f1-score   support\n\n     OFFENSE       0.55      0.65      0.60       497\n       OTHER       0.81      0.74      0.77      1006\n\n    accuracy                           0.71      1503\n   macro avg       0.68      0.70      0.69      1503\nweighted avg       0.73      0.71      0.71      1503\n\n"
    }
   ],
   "source": [
    "# print report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try with word-level n-gram `tf-idf`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = Pipeline([('vect', CountVectorizer(analyzer = \"word\", token_pattern = r\"\\w{1,}\", ngram_range = (2,3))), # ngrams counts\n",
    "                   ('tfidf', TfidfTransformer()), # normalized tf-idf\n",
    "                   ('clf', LogisticRegression()), # logistic regression\n",
    "                  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg.fit(X_train, y_train, clf__sample_weight = weights[0 : len(y_train)]) # training, with sample weights\n",
    "y_pred = logreg.predict(X_test) # predict labels of test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "precision    recall  f1-score   support\n\n     OFFENSE       0.54      0.37      0.44       497\n       OTHER       0.73      0.85      0.78      1006\n\n    accuracy                           0.69      1503\n   macro avg       0.64      0.61      0.61      1503\nweighted avg       0.67      0.69      0.67      1503\n\n"
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try with n-gram character-level `tf-idf`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = Pipeline([('vect', CountVectorizer(analyzer = \"char\", token_pattern = r\"\\w{1,}\", ngram_range = (2,3))), # char-level ngrams counts\n",
    "                   ('tfidf', TfidfTransformer()), # normalized tf-idf\n",
    "                   ('clf', LogisticRegression()), # logistic regression\n",
    "                  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg.fit(X_train, y_train, clf__sample_weight = weights[0 : len(y_train)]) # training, with sample weights\n",
    "y_pred = logreg.predict(X_test) # predict labels of test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "precision    recall  f1-score   support\n\n     OFFENSE       0.60      0.71      0.65       497\n       OTHER       0.84      0.77      0.80      1006\n\n    accuracy                           0.75      1503\n   macro avg       0.72      0.74      0.73      1503\nweighted avg       0.76      0.75      0.75      1503\n\n"
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slightly better!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about Naive Bayes classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = Pipeline([('vect', CountVectorizer()), # token counts\n",
    "               ('tfidf', TfidfTransformer()), # normalized tf-idf\n",
    "               ('clf', MultinomialNB()), # naive bayes classifier\n",
    "               ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb.fit(X_train, y_train, clf__sample_weight = weights[0 : len(y_train)]) # training\n",
    "y_pred = nb.predict(X_test) # predict labels of test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "precision    recall  f1-score   support\n\n     OFFENSE       0.55      0.68      0.61       497\n       OTHER       0.82      0.72      0.77      1006\n\n    accuracy                           0.71      1503\n   macro avg       0.69      0.70      0.69      1503\nweighted avg       0.73      0.71      0.72      1503\n\n"
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's try Naive Bayes with different features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = Pipeline([('vect', CountVectorizer(analyzer = \"char\", token_pattern = r\"\\w{1,}\", ngram_range = (2,3))), # char-level ngrams counts\n",
    "               ('tfidf', TfidfTransformer()), # normalized tf-idf\n",
    "               ('clf', MultinomialNB()), # naive bayes classifier\n",
    "               ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb.fit(X_train, y_train, clf__sample_weight = weights[0 : len(y_train)]) # training\n",
    "y_pred = nb.predict(X_test) # predict labels of test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "precision    recall  f1-score   support\n\n     OFFENSE       0.51      0.87      0.64       497\n       OTHER       0.90      0.59      0.71      1006\n\n    accuracy                           0.68      1503\n   macro avg       0.71      0.73      0.68      1503\nweighted avg       0.77      0.68      0.69      1503\n\n"
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: the best result was obtained with character-level n-grams and logistic regression. Can we improve on that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}